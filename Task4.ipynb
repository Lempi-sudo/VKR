{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cb59f50",
   "metadata": {},
   "source": [
    "####  MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd76e800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from ImageWork import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import cv2\n",
    "from sklearn.preprocessing import StandardScaler  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c523317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pobitovo_sravnenie_WaterMark(W1, W2 , total_bit=1024 ):\n",
    "    t1 = W1==W2\n",
    "    sum=t1.sum()\n",
    "    return sum/total_bit*100\n",
    "\n",
    "def psnr(W, Wr):\n",
    "    e = (np.sum((W - Wr) ** 2)) / (len(W) * len(W))\n",
    "    p = 10 * np.log10(255 ** 2 / e)\n",
    "    return p\n",
    "    \n",
    "\n",
    "def psnrCV2(W, Wr):\n",
    "    p=cv2.PSNR(W,Wr)\n",
    "    return p\n",
    "\n",
    "\n",
    "def one_WM(DataFrame , path_save , path_water_mark , clf):\n",
    "    del DataFrame['Unnamed: 0']\n",
    "    X = DataFrame.iloc[:,[1,2,3,4]].values  \n",
    "    Y=DataFrame.iloc[:,[0]].values\n",
    "    y_predict_image=clf.predict(X)\n",
    "    y_predict_image[y_predict_image>0]=255\n",
    "    y_predict_image=y_predict_image.reshape(32,32)\n",
    "    img = Image.fromarray(y_predict_image.astype(np.uint8))\n",
    "    img.show()\n",
    "    img.save(path_save)\n",
    "    W=WaterMarkLoader.load(path_water_mark)\n",
    "    WR=WaterMarkLoader.load(path_save)\n",
    "    print(\"побитовое сравнение = \", pobitovo_sravnenie_WaterMark(W,WR))\n",
    "    \n",
    "\n",
    "def NO_AttackModel_Test(dataframe,path_save , title , clf, path_water_mark=\"Water Mark Image/WaterMarkRandom.jpg\", index_image=1):\n",
    "    X_dataframe = dataframe.iloc[:,[1,2,3,4]].values  \n",
    "    Y_dataframe = dataframe.iloc[:,[0]].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_dataframe, Y_dataframe, test_size=0.4, random_state=42)\n",
    "    scaler = StandardScaler()  \n",
    "    # Don't cheat - fit only on training data\n",
    "    scaler.fit(X_train)  \n",
    "    X_train = scaler.transform(X_train)  \n",
    "    # apply same transformation to test data\n",
    "    X_test = scaler.transform(X_test)  \n",
    "    print(title)\n",
    "    y_predict=clf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_predict)\n",
    "    print(pd.DataFrame(cm))\n",
    "    print()\n",
    "    print(classification_report(y_test, y_predict))\n",
    "    print()\n",
    "    X_image_test = dataframe.iloc[:,[1,2,3,4]].values  \n",
    "    i=index_image\n",
    "    y=X_image_test[1024*i:1024*(i+1)]\n",
    "    y_predict_image=clf.predict(y)\n",
    "    y_predict_image[y_predict_image>0]=255\n",
    "    y_predict_image=y_predict_image.reshape(32,32)\n",
    "    img = Image.fromarray(y_predict_image.astype(np.uint8))\n",
    "    #img.show()\n",
    "    img.save(path_save)\n",
    "    W=WaterMarkLoader.load(path_water_mark)\n",
    "    WR=WaterMarkLoader.load(path_save)\n",
    "    print(\"побитовое сравнение = \", pobitovo_sravnenie_WaterMark(W,WR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bfaffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "No_Attack_frame = pd.read_csv('feature_vec/NO_Attack_Dota.txt') # ПАРАМЕТР НА ВХОД ФУНКЦИИ\n",
    "del No_Attack_frame['Unnamed: 0']\n",
    "X = No_Attack_frame.iloc[:,[1,2,3,4]].values  \n",
    "Y=No_Attack_frame.iloc[:,[0]].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.35, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "213d86bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.67687682\n",
      "Validation score: 0.736779\n",
      "Iteration 2, loss = 0.48956353\n",
      "Validation score: 0.772686\n",
      "Iteration 3, loss = 0.41472461\n",
      "Validation score: 0.823317\n",
      "Iteration 4, loss = 0.35264188\n",
      "Validation score: 0.873197\n",
      "Iteration 5, loss = 0.29046496\n",
      "Validation score: 0.923077\n",
      "Iteration 6, loss = 0.22978534\n",
      "Validation score: 0.945162\n",
      "Iteration 7, loss = 0.17928255\n",
      "Validation score: 0.955379\n",
      "Iteration 8, loss = 0.15323149\n",
      "Validation score: 0.961388\n",
      "Iteration 9, loss = 0.13501748\n",
      "Validation score: 0.965745\n",
      "Iteration 10, loss = 0.12319595\n",
      "Validation score: 0.968450\n",
      "Iteration 11, loss = 0.11234040\n",
      "Validation score: 0.970252\n",
      "Iteration 12, loss = 0.10530344\n",
      "Validation score: 0.972206\n",
      "Iteration 13, loss = 0.09851793\n",
      "Validation score: 0.974159\n",
      "Iteration 14, loss = 0.09305568\n",
      "Validation score: 0.974609\n",
      "Iteration 15, loss = 0.09039708\n",
      "Validation score: 0.976262\n",
      "Iteration 16, loss = 0.08911649\n",
      "Validation score: 0.976863\n",
      "Iteration 17, loss = 0.08402793\n",
      "Validation score: 0.976112\n",
      "Iteration 18, loss = 0.08323675\n",
      "Validation score: 0.975210\n",
      "Iteration 19, loss = 0.08092691\n",
      "Validation score: 0.978816\n",
      "Iteration 20, loss = 0.07806711\n",
      "Validation score: 0.979267\n",
      "Iteration 21, loss = 0.07648132\n",
      "Validation score: 0.977764\n",
      "Iteration 22, loss = 0.07442368\n",
      "Validation score: 0.978666\n",
      "Iteration 23, loss = 0.07382873\n",
      "Validation score: 0.978966\n",
      "Iteration 24, loss = 0.07256050\n",
      "Validation score: 0.978966\n",
      "Iteration 25, loss = 0.07086348\n",
      "Validation score: 0.977614\n",
      "Iteration 26, loss = 0.07094985\n",
      "Validation score: 0.977314\n",
      "Iteration 27, loss = 0.07034859\n",
      "Validation score: 0.978816\n",
      "Iteration 28, loss = 0.06889903\n",
      "Validation score: 0.978966\n",
      "Iteration 29, loss = 0.06880917\n",
      "Validation score: 0.977464\n",
      "Iteration 30, loss = 0.06745313\n",
      "Validation score: 0.979567\n",
      "Iteration 31, loss = 0.06787495\n",
      "Validation score: 0.978666\n",
      "Iteration 32, loss = 0.06689910\n",
      "Validation score: 0.978666\n",
      "Iteration 33, loss = 0.06544190\n",
      "Validation score: 0.978816\n",
      "Iteration 34, loss = 0.06760215\n",
      "Validation score: 0.978666\n",
      "Iteration 35, loss = 0.06588857\n",
      "Validation score: 0.978516\n",
      "Iteration 36, loss = 0.06582181\n",
      "Validation score: 0.978215\n",
      "Iteration 37, loss = 0.06458018\n",
      "Validation score: 0.980469\n",
      "Iteration 38, loss = 0.06332013\n",
      "Validation score: 0.978666\n",
      "Iteration 39, loss = 0.06482250\n",
      "Validation score: 0.980168\n",
      "Iteration 40, loss = 0.06340501\n",
      "Validation score: 0.979868\n",
      "Iteration 41, loss = 0.06438617\n",
      "Validation score: 0.976262\n",
      "Iteration 42, loss = 0.06548284\n",
      "Validation score: 0.979567\n",
      "Iteration 43, loss = 0.06150386\n",
      "Validation score: 0.979117\n",
      "Iteration 44, loss = 0.06201307\n",
      "Validation score: 0.977163\n",
      "Iteration 45, loss = 0.06091738\n",
      "Validation score: 0.979117\n",
      "Iteration 46, loss = 0.06133681\n",
      "Validation score: 0.978966\n",
      "Iteration 47, loss = 0.06112226\n",
      "Validation score: 0.979417\n",
      "Iteration 48, loss = 0.06372291\n",
      "Validation score: 0.978516\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(early_stopping=True, hidden_layer_sizes=(10, 10, 10, 10),\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes = (10,10,10,10) , max_iter=200 ,early_stopping=True , verbose=True)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a79df305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0     1\n",
      "0  25880   410\n",
      "1    276  9274\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99     26290\n",
      "           1       0.96      0.97      0.96      9550\n",
      "\n",
      "    accuracy                           0.98     35840\n",
      "   macro avg       0.97      0.98      0.98     35840\n",
      "weighted avg       0.98      0.98      0.98     35840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predict=clf.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "print(pd.DataFrame(cm))\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c4be10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.78302950\n",
      "Iteration 2, loss = 0.64951501\n",
      "Iteration 3, loss = 0.61056790\n",
      "Iteration 4, loss = 0.58123290\n",
      "Iteration 5, loss = 0.55709117\n",
      "Iteration 6, loss = 0.53102234\n",
      "Iteration 7, loss = 0.50306112\n",
      "Iteration 8, loss = 0.47167432\n",
      "Iteration 9, loss = 0.44141483\n",
      "Iteration 10, loss = 0.41542480\n",
      "Iteration 11, loss = 0.39404824\n",
      "Iteration 12, loss = 0.37941223\n",
      "Iteration 13, loss = 0.36904901\n",
      "Iteration 14, loss = 0.35958403\n",
      "Iteration 15, loss = 0.35191061\n",
      "Iteration 16, loss = 0.34494368\n",
      "Iteration 17, loss = 0.34073772\n",
      "Iteration 18, loss = 0.33770951\n",
      "Iteration 19, loss = 0.33563705\n",
      "Iteration 20, loss = 0.33406585\n",
      "Iteration 21, loss = 0.33282172\n",
      "Iteration 22, loss = 0.33122595\n",
      "Iteration 23, loss = 0.33041726\n",
      "Iteration 24, loss = 0.32921928\n",
      "Iteration 25, loss = 0.32848866\n",
      "Iteration 26, loss = 0.32755584\n",
      "Iteration 27, loss = 0.32574839\n",
      "Iteration 28, loss = 0.32518537\n",
      "Iteration 29, loss = 0.32386884\n",
      "Iteration 30, loss = 0.32242705\n",
      "Iteration 31, loss = 0.32189987\n",
      "Iteration 32, loss = 0.32006907\n",
      "Iteration 33, loss = 0.31675613\n",
      "Iteration 34, loss = 0.31449020\n",
      "Iteration 35, loss = 0.30993018\n",
      "Iteration 36, loss = 0.30645695\n",
      "Iteration 37, loss = 0.30279792\n",
      "Iteration 38, loss = 0.29765446\n",
      "Iteration 39, loss = 0.29467044\n",
      "Iteration 40, loss = 0.29167820\n",
      "Iteration 41, loss = 0.28783132\n",
      "Iteration 42, loss = 0.28378149\n",
      "Iteration 43, loss = 0.27995080\n",
      "Iteration 44, loss = 0.27607078\n",
      "Iteration 45, loss = 0.27135797\n",
      "Iteration 46, loss = 0.26652728\n",
      "Iteration 47, loss = 0.26193940\n",
      "Iteration 48, loss = 0.25527355\n",
      "Iteration 49, loss = 0.24915778\n",
      "Iteration 50, loss = 0.24456140\n",
      "Iteration 51, loss = 0.24274756\n",
      "Iteration 52, loss = 0.24055177\n",
      "Iteration 53, loss = 0.23841151\n",
      "Iteration 54, loss = 0.23542618\n",
      "Iteration 55, loss = 0.23428981\n",
      "Iteration 56, loss = 0.23184396\n",
      "Iteration 57, loss = 0.23066895\n",
      "Iteration 58, loss = 0.22928608\n",
      "Iteration 59, loss = 0.22832471\n",
      "Iteration 60, loss = 0.22587354\n",
      "Iteration 61, loss = 0.22531774\n",
      "Iteration 62, loss = 0.22317522\n",
      "Iteration 63, loss = 0.22061486\n",
      "Iteration 64, loss = 0.21947607\n",
      "Iteration 65, loss = 0.21720115\n",
      "Iteration 66, loss = 0.21531576\n",
      "Iteration 67, loss = 0.21344137\n",
      "Iteration 68, loss = 0.21310159\n",
      "Iteration 69, loss = 0.21135359\n",
      "Iteration 70, loss = 0.20983401\n",
      "Iteration 71, loss = 0.20881313\n",
      "Iteration 72, loss = 0.20814243\n",
      "Iteration 73, loss = 0.20816854\n",
      "Iteration 74, loss = 0.20646786\n",
      "Iteration 75, loss = 0.20625982\n",
      "Iteration 76, loss = 0.20577335\n",
      "Iteration 77, loss = 0.20537352\n",
      "Iteration 78, loss = 0.20531954\n",
      "Iteration 79, loss = 0.20493815\n",
      "Iteration 80, loss = 0.20455396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.91758202\n",
      "Iteration 2, loss = 0.65663904\n",
      "Iteration 3, loss = 0.61963131\n",
      "Iteration 4, loss = 0.60086387\n",
      "Iteration 5, loss = 0.58564652\n",
      "Iteration 6, loss = 0.56956901\n",
      "Iteration 7, loss = 0.54910098\n",
      "Iteration 8, loss = 0.52037004\n",
      "Iteration 9, loss = 0.49198565\n",
      "Iteration 10, loss = 0.46885036\n",
      "Iteration 11, loss = 0.44988307\n",
      "Iteration 12, loss = 0.43356175\n",
      "Iteration 13, loss = 0.41849982\n",
      "Iteration 14, loss = 0.40462016\n",
      "Iteration 15, loss = 0.38729098\n",
      "Iteration 16, loss = 0.37518009\n",
      "Iteration 17, loss = 0.36908461\n",
      "Iteration 18, loss = 0.36404621\n",
      "Iteration 19, loss = 0.35995324\n",
      "Iteration 20, loss = 0.35749775\n",
      "Iteration 21, loss = 0.35488513\n",
      "Iteration 22, loss = 0.35332688\n",
      "Iteration 23, loss = 0.35043247\n",
      "Iteration 24, loss = 0.35027039\n",
      "Iteration 25, loss = 0.34816920\n",
      "Iteration 26, loss = 0.34707979\n",
      "Iteration 27, loss = 0.34620925\n",
      "Iteration 28, loss = 0.34468941\n",
      "Iteration 29, loss = 0.34341811\n",
      "Iteration 30, loss = 0.34237851\n",
      "Iteration 31, loss = 0.34171401\n",
      "Iteration 32, loss = 0.34088290\n",
      "Iteration 33, loss = 0.33983311\n",
      "Iteration 34, loss = 0.33840989\n",
      "Iteration 35, loss = 0.33473747\n",
      "Iteration 36, loss = 0.33266694\n",
      "Iteration 37, loss = 0.32958079\n",
      "Iteration 38, loss = 0.32502078\n",
      "Iteration 39, loss = 0.32119481\n",
      "Iteration 40, loss = 0.31886545\n",
      "Iteration 41, loss = 0.31599390\n",
      "Iteration 42, loss = 0.31519314\n",
      "Iteration 43, loss = 0.31370096\n",
      "Iteration 44, loss = 0.31263651\n",
      "Iteration 45, loss = 0.31150309\n",
      "Iteration 46, loss = 0.30968283\n",
      "Iteration 47, loss = 0.30954010\n",
      "Iteration 48, loss = 0.30859363\n",
      "Iteration 49, loss = 0.30664374\n",
      "Iteration 50, loss = 0.30678171\n",
      "Iteration 51, loss = 0.30491962\n",
      "Iteration 52, loss = 0.30450019\n",
      "Iteration 53, loss = 0.30462472\n",
      "Iteration 54, loss = 0.30278123\n",
      "Iteration 55, loss = 0.30179125\n",
      "Iteration 56, loss = 0.30189851\n",
      "Iteration 57, loss = 0.30158175\n",
      "Iteration 58, loss = 0.30084797\n",
      "Iteration 59, loss = 0.30033637\n",
      "Iteration 60, loss = 0.29916109\n",
      "Iteration 61, loss = 0.29977410\n",
      "Iteration 62, loss = 0.29894552\n",
      "Iteration 63, loss = 0.29791477\n",
      "Iteration 64, loss = 0.29811358\n",
      "Iteration 65, loss = 0.29848632\n",
      "Iteration 66, loss = 0.29674221\n",
      "Iteration 67, loss = 0.29777739\n",
      "Iteration 68, loss = 0.29675968\n",
      "Iteration 69, loss = 0.29653933\n",
      "Iteration 70, loss = 0.29635646\n",
      "Iteration 71, loss = 0.29766922\n",
      "Iteration 72, loss = 0.29576975\n",
      "Iteration 73, loss = 0.29479458\n",
      "Iteration 74, loss = 0.29603293\n",
      "Iteration 75, loss = 0.29579404\n",
      "Iteration 76, loss = 0.29626732\n",
      "Iteration 77, loss = 0.29399452\n",
      "Iteration 78, loss = 0.29553719\n",
      "Iteration 79, loss = 0.29475643\n",
      "Iteration 80, loss = 0.29511720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.71644966\n",
      "Iteration 2, loss = 0.67316309\n",
      "Iteration 3, loss = 0.66341286\n",
      "Iteration 4, loss = 0.65096255\n",
      "Iteration 5, loss = 0.63564858\n",
      "Iteration 6, loss = 0.61593949\n",
      "Iteration 7, loss = 0.59732886\n",
      "Iteration 8, loss = 0.58198177\n",
      "Iteration 9, loss = 0.56855144\n",
      "Iteration 10, loss = 0.55852122\n",
      "Iteration 11, loss = 0.55024971\n",
      "Iteration 12, loss = 0.54317076\n",
      "Iteration 13, loss = 0.53720414\n",
      "Iteration 14, loss = 0.53001088\n",
      "Iteration 15, loss = 0.52181000\n",
      "Iteration 16, loss = 0.51523315\n",
      "Iteration 17, loss = 0.50553901\n",
      "Iteration 18, loss = 0.49834222\n",
      "Iteration 19, loss = 0.49290693\n",
      "Iteration 20, loss = 0.48909606\n",
      "Iteration 21, loss = 0.48658454\n",
      "Iteration 22, loss = 0.48349976\n",
      "Iteration 23, loss = 0.48124133\n",
      "Iteration 24, loss = 0.47858917\n",
      "Iteration 25, loss = 0.47794818\n",
      "Iteration 26, loss = 0.47618035\n",
      "Iteration 27, loss = 0.47522090\n",
      "Iteration 28, loss = 0.47399516\n",
      "Iteration 29, loss = 0.47205234\n",
      "Iteration 30, loss = 0.47176633\n",
      "Iteration 31, loss = 0.47086858\n",
      "Iteration 32, loss = 0.46947241\n",
      "Iteration 33, loss = 0.46848876\n",
      "Iteration 34, loss = 0.46813575\n",
      "Iteration 35, loss = 0.46727960\n",
      "Iteration 36, loss = 0.46581323\n",
      "Iteration 37, loss = 0.46536150\n",
      "Iteration 38, loss = 0.46502594\n",
      "Iteration 39, loss = 0.46496795\n",
      "Iteration 40, loss = 0.46429981\n",
      "Iteration 41, loss = 0.46363950\n",
      "Iteration 42, loss = 0.46379981\n",
      "Iteration 43, loss = 0.46389241\n",
      "Iteration 44, loss = 0.46225751\n",
      "Iteration 45, loss = 0.46165018\n",
      "Iteration 46, loss = 0.46136225\n",
      "Iteration 47, loss = 0.46180017\n",
      "Iteration 48, loss = 0.46044034\n",
      "Iteration 49, loss = 0.46056994\n",
      "Iteration 50, loss = 0.45973698\n",
      "Iteration 51, loss = 0.45961876\n",
      "Iteration 52, loss = 0.45978214\n",
      "Iteration 53, loss = 0.45909094\n",
      "Iteration 54, loss = 0.45853090\n",
      "Iteration 55, loss = 0.45839183\n",
      "Iteration 56, loss = 0.46068649\n",
      "Iteration 57, loss = 0.45844652\n",
      "Iteration 58, loss = 0.45783279\n",
      "Iteration 59, loss = 0.45777749\n",
      "Iteration 60, loss = 0.45711996\n",
      "Iteration 61, loss = 0.45711138\n",
      "Iteration 62, loss = 0.45633539\n",
      "Iteration 63, loss = 0.45628505\n",
      "Iteration 64, loss = 0.45710778\n",
      "Iteration 65, loss = 0.45646029\n",
      "Iteration 66, loss = 0.45552968\n",
      "Iteration 67, loss = 0.45528441\n",
      "Iteration 68, loss = 0.45580355\n",
      "Iteration 69, loss = 0.45480599\n",
      "Iteration 70, loss = 0.45472951\n",
      "Iteration 71, loss = 0.45404616\n",
      "Iteration 72, loss = 0.45505293\n",
      "Iteration 73, loss = 0.45428523\n",
      "Iteration 74, loss = 0.45367631\n",
      "Iteration 75, loss = 0.45392330\n",
      "Iteration 76, loss = 0.45536807\n",
      "Iteration 77, loss = 0.45367901\n",
      "Iteration 78, loss = 0.45371309\n",
      "Iteration 79, loss = 0.45315068\n",
      "Iteration 80, loss = 0.45300763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.73545242\n",
      "Iteration 2, loss = 0.67076637\n",
      "Iteration 3, loss = 0.64630667\n",
      "Iteration 4, loss = 0.62803758\n",
      "Iteration 5, loss = 0.61069578\n",
      "Iteration 6, loss = 0.59461514\n",
      "Iteration 7, loss = 0.57756427\n",
      "Iteration 8, loss = 0.56128648\n",
      "Iteration 9, loss = 0.54611701\n",
      "Iteration 10, loss = 0.52921731\n",
      "Iteration 11, loss = 0.51302789\n",
      "Iteration 12, loss = 0.50044202\n",
      "Iteration 13, loss = 0.48927315\n",
      "Iteration 14, loss = 0.48033620\n",
      "Iteration 15, loss = 0.47417831\n",
      "Iteration 16, loss = 0.46984356\n",
      "Iteration 17, loss = 0.46551046\n",
      "Iteration 18, loss = 0.46178008\n",
      "Iteration 19, loss = 0.45899436\n",
      "Iteration 20, loss = 0.45628508\n",
      "Iteration 21, loss = 0.45551185\n",
      "Iteration 22, loss = 0.45247411\n",
      "Iteration 23, loss = 0.45061494\n",
      "Iteration 24, loss = 0.44918841\n",
      "Iteration 25, loss = 0.44744532\n",
      "Iteration 26, loss = 0.44444870\n",
      "Iteration 27, loss = 0.43979008\n",
      "Iteration 28, loss = 0.43455222\n",
      "Iteration 29, loss = 0.42753353\n",
      "Iteration 30, loss = 0.42073888\n",
      "Iteration 31, loss = 0.41497004\n",
      "Iteration 32, loss = 0.41028825\n",
      "Iteration 33, loss = 0.40807603\n",
      "Iteration 34, loss = 0.40383672\n",
      "Iteration 35, loss = 0.40009752\n",
      "Iteration 36, loss = 0.39806018\n",
      "Iteration 37, loss = 0.39600296\n",
      "Iteration 38, loss = 0.39404417\n",
      "Iteration 39, loss = 0.39349601\n",
      "Iteration 40, loss = 0.39121856\n",
      "Iteration 41, loss = 0.38999048\n",
      "Iteration 42, loss = 0.38730374\n",
      "Iteration 43, loss = 0.38559588\n",
      "Iteration 44, loss = 0.38454973\n",
      "Iteration 45, loss = 0.38424052\n",
      "Iteration 46, loss = 0.38177260\n",
      "Iteration 47, loss = 0.38079936\n",
      "Iteration 48, loss = 0.38092819\n",
      "Iteration 49, loss = 0.38010555\n",
      "Iteration 50, loss = 0.37913241\n",
      "Iteration 51, loss = 0.37810310\n",
      "Iteration 52, loss = 0.37714744\n",
      "Iteration 53, loss = 0.37710215\n",
      "Iteration 54, loss = 0.37632148\n",
      "Iteration 55, loss = 0.37697115\n",
      "Iteration 56, loss = 0.37473653\n",
      "Iteration 57, loss = 0.37499328\n",
      "Iteration 58, loss = 0.37315771\n",
      "Iteration 59, loss = 0.37359582\n",
      "Iteration 60, loss = 0.37268831\n",
      "Iteration 61, loss = 0.37261733\n",
      "Iteration 62, loss = 0.37043208\n",
      "Iteration 63, loss = 0.37265021\n",
      "Iteration 64, loss = 0.36994447\n",
      "Iteration 65, loss = 0.37288256\n",
      "Iteration 66, loss = 0.36943000\n",
      "Iteration 67, loss = 0.37032869\n",
      "Iteration 68, loss = 0.36886516\n",
      "Iteration 69, loss = 0.36968310\n",
      "Iteration 70, loss = 0.37022073\n",
      "Iteration 71, loss = 0.36636162\n",
      "Iteration 72, loss = 0.36755627\n",
      "Iteration 73, loss = 0.36686102\n",
      "Iteration 74, loss = 0.37056784\n",
      "Iteration 75, loss = 0.36644120\n",
      "Iteration 76, loss = 0.36993852\n",
      "Iteration 77, loss = 0.36334034\n",
      "Iteration 78, loss = 0.36396571\n",
      "Iteration 79, loss = 0.36243790\n",
      "Iteration 80, loss = 0.35818927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.73486699\n",
      "Iteration 2, loss = 0.68150611\n",
      "Iteration 3, loss = 0.67227311\n",
      "Iteration 4, loss = 0.66565003\n",
      "Iteration 5, loss = 0.66037554\n",
      "Iteration 6, loss = 0.65596591\n",
      "Iteration 7, loss = 0.65131442\n",
      "Iteration 8, loss = 0.64362959\n",
      "Iteration 9, loss = 0.63547146\n",
      "Iteration 10, loss = 0.62851456\n",
      "Iteration 11, loss = 0.62208148\n",
      "Iteration 12, loss = 0.61641135\n",
      "Iteration 13, loss = 0.61043464\n",
      "Iteration 14, loss = 0.60353465\n",
      "Iteration 15, loss = 0.59469674\n",
      "Iteration 16, loss = 0.58348504\n",
      "Iteration 17, loss = 0.56903477\n",
      "Iteration 18, loss = 0.55063758\n",
      "Iteration 19, loss = 0.53443390\n",
      "Iteration 20, loss = 0.51706228\n",
      "Iteration 21, loss = 0.49583402\n",
      "Iteration 22, loss = 0.46396306\n",
      "Iteration 23, loss = 0.42528758\n",
      "Iteration 24, loss = 0.38938924\n",
      "Iteration 25, loss = 0.36889668\n",
      "Iteration 26, loss = 0.35426093\n",
      "Iteration 27, loss = 0.34382997\n",
      "Iteration 28, loss = 0.33465389\n",
      "Iteration 29, loss = 0.32942045\n",
      "Iteration 30, loss = 0.32092620\n",
      "Iteration 31, loss = 0.31588820\n",
      "Iteration 32, loss = 0.31289137\n",
      "Iteration 33, loss = 0.31059599\n",
      "Iteration 34, loss = 0.30733717\n",
      "Iteration 35, loss = 0.30668503\n",
      "Iteration 36, loss = 0.30165953\n",
      "Iteration 37, loss = 0.30084538\n",
      "Iteration 38, loss = 0.29916627\n",
      "Iteration 39, loss = 0.29673041\n",
      "Iteration 40, loss = 0.29527946\n",
      "Iteration 41, loss = 0.29287216\n",
      "Iteration 42, loss = 0.29215902\n",
      "Iteration 43, loss = 0.28938686\n",
      "Iteration 44, loss = 0.28964978\n",
      "Iteration 45, loss = 0.28669413\n",
      "Iteration 46, loss = 0.28618452\n",
      "Iteration 47, loss = 0.28376707\n",
      "Iteration 48, loss = 0.28325574\n",
      "Iteration 49, loss = 0.28099724\n",
      "Iteration 50, loss = 0.27995056\n",
      "Iteration 51, loss = 0.27666097\n",
      "Iteration 52, loss = 0.27733616\n",
      "Iteration 53, loss = 0.27736102\n",
      "Iteration 54, loss = 0.27565025\n",
      "Iteration 55, loss = 0.27471736\n",
      "Iteration 56, loss = 0.27454173\n",
      "Iteration 57, loss = 0.27115126\n",
      "Iteration 58, loss = 0.27104415\n",
      "Iteration 59, loss = 0.27002122\n",
      "Iteration 60, loss = 0.26837778\n",
      "Iteration 61, loss = 0.27113503\n",
      "Iteration 62, loss = 0.27001470\n",
      "Iteration 63, loss = 0.26878675\n",
      "Iteration 64, loss = 0.26625316\n",
      "Iteration 65, loss = 0.26579065\n",
      "Iteration 66, loss = 0.26483678\n",
      "Iteration 67, loss = 0.26456944\n",
      "Iteration 68, loss = 0.26557161\n",
      "Iteration 69, loss = 0.26655284\n",
      "Iteration 70, loss = 0.26259468\n",
      "Iteration 71, loss = 0.26115867\n",
      "Iteration 72, loss = 0.26297532\n",
      "Iteration 73, loss = 0.26083688\n",
      "Iteration 74, loss = 0.26030229\n",
      "Iteration 75, loss = 0.26298324\n",
      "Iteration 76, loss = 0.26125306\n",
      "Iteration 77, loss = 0.26050395\n",
      "Iteration 78, loss = 0.25748246\n",
      "Iteration 79, loss = 0.25901176\n",
      "Iteration 80, loss = 0.25835822\n",
      "Iteration 1, loss = 0.82025612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.70221423\n",
      "Iteration 3, loss = 0.67545760\n",
      "Iteration 4, loss = 0.65110400\n",
      "Iteration 5, loss = 0.58736688\n",
      "Iteration 6, loss = 0.51032092\n",
      "Iteration 7, loss = 0.45101457\n",
      "Iteration 8, loss = 0.40444631\n",
      "Iteration 9, loss = 0.35647478\n",
      "Iteration 10, loss = 0.30900499\n",
      "Iteration 11, loss = 0.27231892\n",
      "Iteration 12, loss = 0.24805080\n",
      "Iteration 13, loss = 0.22956370\n",
      "Iteration 14, loss = 0.21548528\n",
      "Iteration 15, loss = 0.20597968\n",
      "Iteration 16, loss = 0.19591580\n",
      "Iteration 17, loss = 0.18667767\n",
      "Iteration 18, loss = 0.18036645\n",
      "Iteration 19, loss = 0.17403684\n",
      "Iteration 20, loss = 0.16943512\n",
      "Iteration 21, loss = 0.16571720\n",
      "Iteration 22, loss = 0.16012568\n",
      "Iteration 23, loss = 0.15986195\n",
      "Iteration 24, loss = 0.15368555\n",
      "Iteration 25, loss = 0.15159620\n",
      "Iteration 26, loss = 0.14844119\n",
      "Iteration 27, loss = 0.14527731\n",
      "Iteration 28, loss = 0.14277362\n",
      "Iteration 29, loss = 0.14222808\n",
      "Iteration 30, loss = 0.13949919\n",
      "Iteration 31, loss = 0.13993244\n",
      "Iteration 32, loss = 0.13686403\n",
      "Iteration 33, loss = 0.13646599\n",
      "Iteration 34, loss = 0.13711165\n",
      "Iteration 35, loss = 0.13301581\n",
      "Iteration 36, loss = 0.13274954\n",
      "Iteration 37, loss = 0.13320789\n",
      "Iteration 38, loss = 0.13214462\n",
      "Iteration 39, loss = 0.13019715\n",
      "Iteration 40, loss = 0.12939324\n",
      "Iteration 41, loss = 0.12752719\n",
      "Iteration 42, loss = 0.12809483\n",
      "Iteration 43, loss = 0.12614901\n",
      "Iteration 44, loss = 0.12718390\n",
      "Iteration 45, loss = 0.12557124\n",
      "Iteration 46, loss = 0.12443102\n",
      "Iteration 47, loss = 0.12420490\n",
      "Iteration 48, loss = 0.12383061\n",
      "Iteration 49, loss = 0.12310816\n",
      "Iteration 50, loss = 0.12204036\n",
      "Iteration 51, loss = 0.12229134\n",
      "Iteration 52, loss = 0.12054966\n",
      "Iteration 53, loss = 0.12354849\n",
      "Iteration 54, loss = 0.11894664\n",
      "Iteration 55, loss = 0.11922550\n",
      "Iteration 56, loss = 0.11872874\n",
      "Iteration 57, loss = 0.11907829\n",
      "Iteration 58, loss = 0.11806798\n",
      "Iteration 59, loss = 0.12034865\n",
      "Iteration 60, loss = 0.11655559\n",
      "Iteration 61, loss = 0.11701124\n",
      "Iteration 62, loss = 0.11644969\n",
      "Iteration 63, loss = 0.11754183\n",
      "Iteration 64, loss = 0.11713663\n",
      "Iteration 65, loss = 0.11623270\n",
      "Iteration 66, loss = 0.11753291\n",
      "Iteration 67, loss = 0.11717313\n",
      "Iteration 68, loss = 0.11448108\n",
      "Iteration 69, loss = 0.11434612\n",
      "Iteration 70, loss = 0.11355027\n",
      "Iteration 71, loss = 0.11500659\n",
      "Iteration 72, loss = 0.11544004\n",
      "Iteration 73, loss = 0.11220821\n",
      "Iteration 74, loss = 0.11372107\n",
      "Iteration 75, loss = 0.11390255\n",
      "Iteration 76, loss = 0.11217786\n",
      "Iteration 77, loss = 0.11300909\n",
      "Iteration 78, loss = 0.11209869\n",
      "Iteration 79, loss = 0.11187661\n",
      "Iteration 80, loss = 0.11152414\n",
      "Iteration 1, loss = 0.71667306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.66164808\n",
      "Iteration 3, loss = 0.63111424\n",
      "Iteration 4, loss = 0.57979206\n",
      "Iteration 5, loss = 0.50973774\n",
      "Iteration 6, loss = 0.45819380\n",
      "Iteration 7, loss = 0.41636078\n",
      "Iteration 8, loss = 0.37967456\n",
      "Iteration 9, loss = 0.34882043\n",
      "Iteration 10, loss = 0.32518775\n",
      "Iteration 11, loss = 0.30094686\n",
      "Iteration 12, loss = 0.28807831\n",
      "Iteration 13, loss = 0.27142037\n",
      "Iteration 14, loss = 0.26164445\n",
      "Iteration 15, loss = 0.25122408\n",
      "Iteration 16, loss = 0.24176954\n",
      "Iteration 17, loss = 0.23863908\n",
      "Iteration 18, loss = 0.23317007\n",
      "Iteration 19, loss = 0.22618156\n",
      "Iteration 20, loss = 0.22123648\n",
      "Iteration 21, loss = 0.21574975\n",
      "Iteration 22, loss = 0.21223793\n",
      "Iteration 23, loss = 0.20716028\n",
      "Iteration 24, loss = 0.20561711\n",
      "Iteration 25, loss = 0.20064608\n",
      "Iteration 26, loss = 0.19894604\n",
      "Iteration 27, loss = 0.19335250\n",
      "Iteration 28, loss = 0.19391024\n",
      "Iteration 29, loss = 0.18868372\n",
      "Iteration 30, loss = 0.18806638\n",
      "Iteration 31, loss = 0.18572588\n",
      "Iteration 32, loss = 0.18396178\n",
      "Iteration 33, loss = 0.18326408\n",
      "Iteration 34, loss = 0.17679253\n",
      "Iteration 35, loss = 0.17831639\n",
      "Iteration 36, loss = 0.17525796\n",
      "Iteration 37, loss = 0.17017879\n",
      "Iteration 38, loss = 0.17439043\n",
      "Iteration 39, loss = 0.16777512\n",
      "Iteration 40, loss = 0.16641795\n",
      "Iteration 41, loss = 0.16657165\n",
      "Iteration 42, loss = 0.16268461\n",
      "Iteration 43, loss = 0.16141445\n",
      "Iteration 44, loss = 0.16219979\n",
      "Iteration 45, loss = 0.15917044\n",
      "Iteration 46, loss = 0.15650500\n",
      "Iteration 47, loss = 0.15636488\n",
      "Iteration 48, loss = 0.15771200\n",
      "Iteration 49, loss = 0.15236764\n",
      "Iteration 50, loss = 0.15203469\n",
      "Iteration 51, loss = 0.15286144\n",
      "Iteration 52, loss = 0.15132616\n",
      "Iteration 53, loss = 0.15233073\n",
      "Iteration 54, loss = 0.14989091\n",
      "Iteration 55, loss = 0.14725575\n",
      "Iteration 56, loss = 0.14690122\n",
      "Iteration 57, loss = 0.15166805\n",
      "Iteration 58, loss = 0.14631200\n",
      "Iteration 59, loss = 0.14056313\n",
      "Iteration 60, loss = 0.14333294\n",
      "Iteration 61, loss = 0.14334667\n",
      "Iteration 62, loss = 0.14046599\n",
      "Iteration 63, loss = 0.13985483\n",
      "Iteration 64, loss = 0.14113405\n",
      "Iteration 65, loss = 0.14323267\n",
      "Iteration 66, loss = 0.13757611\n",
      "Iteration 67, loss = 0.13863354\n",
      "Iteration 68, loss = 0.13929891\n",
      "Iteration 69, loss = 0.13960454\n",
      "Iteration 70, loss = 0.13698319\n",
      "Iteration 71, loss = 0.13497997\n",
      "Iteration 72, loss = 0.13590503\n",
      "Iteration 73, loss = 0.13749564\n",
      "Iteration 74, loss = 0.13596117\n",
      "Iteration 75, loss = 0.13661941\n",
      "Iteration 76, loss = 0.13462235\n",
      "Iteration 77, loss = 0.13257195\n",
      "Iteration 78, loss = 0.13189062\n",
      "Iteration 79, loss = 0.13426508\n",
      "Iteration 80, loss = 0.13387853\n",
      "Iteration 1, loss = 0.69827566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.66181234\n",
      "Iteration 3, loss = 0.63979504\n",
      "Iteration 4, loss = 0.61440661\n",
      "Iteration 5, loss = 0.58584789\n",
      "Iteration 6, loss = 0.54600054\n",
      "Iteration 7, loss = 0.51100907\n",
      "Iteration 8, loss = 0.48636206\n",
      "Iteration 9, loss = 0.46644249\n",
      "Iteration 10, loss = 0.44926109\n",
      "Iteration 11, loss = 0.43280033\n",
      "Iteration 12, loss = 0.41911443\n",
      "Iteration 13, loss = 0.40596469\n",
      "Iteration 14, loss = 0.39334260\n",
      "Iteration 15, loss = 0.37891162\n",
      "Iteration 16, loss = 0.36720599\n",
      "Iteration 17, loss = 0.35518480\n",
      "Iteration 18, loss = 0.34321232\n",
      "Iteration 19, loss = 0.33274880\n",
      "Iteration 20, loss = 0.32353635\n",
      "Iteration 21, loss = 0.31514919\n",
      "Iteration 22, loss = 0.30733336\n",
      "Iteration 23, loss = 0.30023478\n",
      "Iteration 24, loss = 0.29635319\n",
      "Iteration 25, loss = 0.29037656\n",
      "Iteration 26, loss = 0.28508152\n",
      "Iteration 27, loss = 0.28046158\n",
      "Iteration 28, loss = 0.27753318\n",
      "Iteration 29, loss = 0.27240726\n",
      "Iteration 30, loss = 0.26862403\n",
      "Iteration 31, loss = 0.26561575\n",
      "Iteration 32, loss = 0.25947158\n",
      "Iteration 33, loss = 0.25501184\n",
      "Iteration 34, loss = 0.24794952\n",
      "Iteration 35, loss = 0.24433928\n",
      "Iteration 36, loss = 0.23764972\n",
      "Iteration 37, loss = 0.23508230\n",
      "Iteration 38, loss = 0.23096723\n",
      "Iteration 39, loss = 0.22604468\n",
      "Iteration 40, loss = 0.22533341\n",
      "Iteration 41, loss = 0.22141532\n",
      "Iteration 42, loss = 0.21572862\n",
      "Iteration 43, loss = 0.21343524\n",
      "Iteration 44, loss = 0.20948499\n",
      "Iteration 45, loss = 0.20655393\n",
      "Iteration 46, loss = 0.20413750\n",
      "Iteration 47, loss = 0.20100479\n",
      "Iteration 48, loss = 0.19711358\n",
      "Iteration 49, loss = 0.19424416\n",
      "Iteration 50, loss = 0.18936900\n",
      "Iteration 51, loss = 0.18474887\n",
      "Iteration 52, loss = 0.18254839\n",
      "Iteration 53, loss = 0.17996849\n",
      "Iteration 54, loss = 0.17919545\n",
      "Iteration 55, loss = 0.17295506\n",
      "Iteration 56, loss = 0.17344801\n",
      "Iteration 57, loss = 0.17139860\n",
      "Iteration 58, loss = 0.16716709\n",
      "Iteration 59, loss = 0.16477821\n",
      "Iteration 60, loss = 0.16345002\n",
      "Iteration 61, loss = 0.16197654\n",
      "Iteration 62, loss = 0.15972251\n",
      "Iteration 63, loss = 0.16152967\n",
      "Iteration 64, loss = 0.15611889\n",
      "Iteration 65, loss = 0.15628496\n",
      "Iteration 66, loss = 0.15340425\n",
      "Iteration 67, loss = 0.15295693\n",
      "Iteration 68, loss = 0.15188722\n",
      "Iteration 69, loss = 0.14978696\n",
      "Iteration 70, loss = 0.14707999\n",
      "Iteration 71, loss = 0.14677719\n",
      "Iteration 72, loss = 0.14544053\n",
      "Iteration 73, loss = 0.14568636\n",
      "Iteration 74, loss = 0.14547960\n",
      "Iteration 75, loss = 0.14300946\n",
      "Iteration 76, loss = 0.14485857\n",
      "Iteration 77, loss = 0.14319851\n",
      "Iteration 78, loss = 0.13997819\n",
      "Iteration 79, loss = 0.14082437\n",
      "Iteration 80, loss = 0.13765639\n",
      "Iteration 1, loss = 0.88222568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.65106352\n",
      "Iteration 3, loss = 0.62143941\n",
      "Iteration 4, loss = 0.59467331\n",
      "Iteration 5, loss = 0.57018652\n",
      "Iteration 6, loss = 0.54181868\n",
      "Iteration 7, loss = 0.51694022\n",
      "Iteration 8, loss = 0.49371868\n",
      "Iteration 9, loss = 0.47517170\n",
      "Iteration 10, loss = 0.45675125\n",
      "Iteration 11, loss = 0.44008055\n",
      "Iteration 12, loss = 0.42212870\n",
      "Iteration 13, loss = 0.40358119\n",
      "Iteration 14, loss = 0.38121078\n",
      "Iteration 15, loss = 0.36083783\n",
      "Iteration 16, loss = 0.34187211\n",
      "Iteration 17, loss = 0.32301961\n",
      "Iteration 18, loss = 0.30336710\n",
      "Iteration 19, loss = 0.29041941\n",
      "Iteration 20, loss = 0.27909867\n",
      "Iteration 21, loss = 0.27113534\n",
      "Iteration 22, loss = 0.26224431\n",
      "Iteration 23, loss = 0.25510292\n",
      "Iteration 24, loss = 0.24775726\n",
      "Iteration 25, loss = 0.24221562\n",
      "Iteration 26, loss = 0.23643908\n",
      "Iteration 27, loss = 0.23561610\n",
      "Iteration 28, loss = 0.22871047\n",
      "Iteration 29, loss = 0.22585081\n",
      "Iteration 30, loss = 0.22279486\n",
      "Iteration 31, loss = 0.21943319\n",
      "Iteration 32, loss = 0.21925488\n",
      "Iteration 33, loss = 0.21160165\n",
      "Iteration 34, loss = 0.21046300\n",
      "Iteration 35, loss = 0.20776711\n",
      "Iteration 36, loss = 0.20429265\n",
      "Iteration 37, loss = 0.20161791\n",
      "Iteration 38, loss = 0.19911660\n",
      "Iteration 39, loss = 0.19908788\n",
      "Iteration 40, loss = 0.19820085\n",
      "Iteration 41, loss = 0.19455477\n",
      "Iteration 42, loss = 0.19258282\n",
      "Iteration 43, loss = 0.19169474\n",
      "Iteration 44, loss = 0.19129370\n",
      "Iteration 45, loss = 0.18949661\n",
      "Iteration 46, loss = 0.18662809\n",
      "Iteration 47, loss = 0.18469920\n",
      "Iteration 48, loss = 0.18266735\n",
      "Iteration 49, loss = 0.18371425\n",
      "Iteration 50, loss = 0.18055357\n",
      "Iteration 51, loss = 0.17976505\n",
      "Iteration 52, loss = 0.17883672\n",
      "Iteration 53, loss = 0.18075137\n",
      "Iteration 54, loss = 0.17770292\n",
      "Iteration 55, loss = 0.17675889\n",
      "Iteration 56, loss = 0.17826092\n",
      "Iteration 57, loss = 0.17445625\n",
      "Iteration 58, loss = 0.17146798\n",
      "Iteration 59, loss = 0.17021024\n",
      "Iteration 60, loss = 0.16982507\n",
      "Iteration 61, loss = 0.17422848\n",
      "Iteration 62, loss = 0.16998936\n",
      "Iteration 63, loss = 0.17031335\n",
      "Iteration 64, loss = 0.16908280\n",
      "Iteration 65, loss = 0.16887633\n",
      "Iteration 66, loss = 0.16736308\n",
      "Iteration 67, loss = 0.16669250\n",
      "Iteration 68, loss = 0.17049215\n",
      "Iteration 69, loss = 0.16418425\n",
      "Iteration 70, loss = 0.16669174\n",
      "Iteration 71, loss = 0.16567481\n",
      "Iteration 72, loss = 0.16231937\n",
      "Iteration 73, loss = 0.16083045\n",
      "Iteration 74, loss = 0.16063807\n",
      "Iteration 75, loss = 0.16249985\n",
      "Iteration 76, loss = 0.16146132\n",
      "Iteration 77, loss = 0.15917504\n",
      "Iteration 78, loss = 0.15780500\n",
      "Iteration 79, loss = 0.16341149\n",
      "Iteration 80, loss = 0.15871800\n",
      "Iteration 1, loss = 0.76867723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.65198673\n",
      "Iteration 3, loss = 0.60259779\n",
      "Iteration 4, loss = 0.56038509\n",
      "Iteration 5, loss = 0.51575503\n",
      "Iteration 6, loss = 0.47865990\n",
      "Iteration 7, loss = 0.44818712\n",
      "Iteration 8, loss = 0.41799980\n",
      "Iteration 9, loss = 0.38086522\n",
      "Iteration 10, loss = 0.34998525\n",
      "Iteration 11, loss = 0.32483920\n",
      "Iteration 12, loss = 0.29928804\n",
      "Iteration 13, loss = 0.27581642\n",
      "Iteration 14, loss = 0.25406584\n",
      "Iteration 15, loss = 0.23994138\n",
      "Iteration 16, loss = 0.23233578\n",
      "Iteration 17, loss = 0.22430421\n",
      "Iteration 18, loss = 0.21560148\n",
      "Iteration 19, loss = 0.20928137\n",
      "Iteration 20, loss = 0.20145270\n",
      "Iteration 21, loss = 0.19614154\n",
      "Iteration 22, loss = 0.18758859\n",
      "Iteration 23, loss = 0.18110219\n",
      "Iteration 24, loss = 0.17445798\n",
      "Iteration 25, loss = 0.17114045\n",
      "Iteration 26, loss = 0.16594680\n",
      "Iteration 27, loss = 0.16448040\n",
      "Iteration 28, loss = 0.15790490\n",
      "Iteration 29, loss = 0.15559695\n",
      "Iteration 30, loss = 0.15269092\n",
      "Iteration 31, loss = 0.14897766\n",
      "Iteration 32, loss = 0.14633379\n",
      "Iteration 33, loss = 0.14455188\n",
      "Iteration 34, loss = 0.14223015\n",
      "Iteration 35, loss = 0.14071880\n",
      "Iteration 36, loss = 0.13903202\n",
      "Iteration 37, loss = 0.13660982\n",
      "Iteration 38, loss = 0.13654744\n",
      "Iteration 39, loss = 0.13489574\n",
      "Iteration 40, loss = 0.13427662\n",
      "Iteration 41, loss = 0.13315197\n",
      "Iteration 42, loss = 0.13119771\n",
      "Iteration 43, loss = 0.13149388\n",
      "Iteration 44, loss = 0.12997158\n",
      "Iteration 45, loss = 0.12880131\n",
      "Iteration 46, loss = 0.12820900\n",
      "Iteration 47, loss = 0.12954393\n",
      "Iteration 48, loss = 0.12644270\n",
      "Iteration 49, loss = 0.12680909\n",
      "Iteration 50, loss = 0.12598257\n",
      "Iteration 51, loss = 0.12595577\n",
      "Iteration 52, loss = 0.12551594\n",
      "Iteration 53, loss = 0.12307524\n",
      "Iteration 54, loss = 0.12347281\n",
      "Iteration 55, loss = 0.12314858\n",
      "Iteration 56, loss = 0.12333664\n",
      "Iteration 57, loss = 0.12301450\n",
      "Iteration 58, loss = 0.12141844\n",
      "Iteration 59, loss = 0.12091165\n",
      "Iteration 60, loss = 0.12121560\n",
      "Iteration 61, loss = 0.12010628\n",
      "Iteration 62, loss = 0.11995636\n",
      "Iteration 63, loss = 0.12030905\n",
      "Iteration 64, loss = 0.11967965\n",
      "Iteration 65, loss = 0.11935611\n",
      "Iteration 66, loss = 0.11860642\n",
      "Iteration 67, loss = 0.11837819\n",
      "Iteration 68, loss = 0.11902146\n",
      "Iteration 69, loss = 0.11935558\n",
      "Iteration 70, loss = 0.11861927\n",
      "Iteration 71, loss = 0.11786369\n",
      "Iteration 72, loss = 0.11656793\n",
      "Iteration 73, loss = 0.11724215\n",
      "Iteration 74, loss = 0.11774384\n",
      "Iteration 75, loss = 0.11642907\n",
      "Iteration 76, loss = 0.11680213\n",
      "Iteration 77, loss = 0.11532872\n",
      "Iteration 78, loss = 0.11519636\n",
      "Iteration 79, loss = 0.11458858\n",
      "Iteration 80, loss = 0.11429112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.68509030\n",
      "Iteration 2, loss = 0.60840205\n",
      "Iteration 3, loss = 0.53350439\n",
      "Iteration 4, loss = 0.44702334\n",
      "Iteration 5, loss = 0.37304673\n",
      "Iteration 6, loss = 0.32490586\n",
      "Iteration 7, loss = 0.27538765\n",
      "Iteration 8, loss = 0.23697534\n",
      "Iteration 9, loss = 0.21517442\n",
      "Iteration 10, loss = 0.20021214\n",
      "Iteration 11, loss = 0.18842781\n",
      "Iteration 12, loss = 0.18090672\n",
      "Iteration 13, loss = 0.17267624\n",
      "Iteration 14, loss = 0.16482643\n",
      "Iteration 15, loss = 0.15799231\n",
      "Iteration 16, loss = 0.15190318\n",
      "Iteration 17, loss = 0.14666099\n",
      "Iteration 18, loss = 0.14290900\n",
      "Iteration 19, loss = 0.13799332\n",
      "Iteration 20, loss = 0.13489865\n",
      "Iteration 21, loss = 0.13167400\n",
      "Iteration 22, loss = 0.12772020\n",
      "Iteration 23, loss = 0.12540386\n",
      "Iteration 24, loss = 0.12302445\n",
      "Iteration 25, loss = 0.12089779\n",
      "Iteration 26, loss = 0.11857274\n",
      "Iteration 27, loss = 0.11617052\n",
      "Iteration 28, loss = 0.11364930\n",
      "Iteration 29, loss = 0.11302425\n",
      "Iteration 30, loss = 0.11227714\n",
      "Iteration 31, loss = 0.10881997\n",
      "Iteration 32, loss = 0.10792625\n",
      "Iteration 33, loss = 0.10692925\n",
      "Iteration 34, loss = 0.10655004\n",
      "Iteration 35, loss = 0.10415187\n",
      "Iteration 36, loss = 0.10343405\n",
      "Iteration 37, loss = 0.10217096\n",
      "Iteration 38, loss = 0.10056232\n",
      "Iteration 39, loss = 0.10174254\n",
      "Iteration 40, loss = 0.10113659\n",
      "Iteration 41, loss = 0.09875670\n",
      "Iteration 42, loss = 0.09927129\n",
      "Iteration 43, loss = 0.09661525\n",
      "Iteration 44, loss = 0.09749684\n",
      "Iteration 45, loss = 0.09653685\n",
      "Iteration 46, loss = 0.09557491\n",
      "Iteration 47, loss = 0.09595561\n",
      "Iteration 48, loss = 0.09606327\n",
      "Iteration 49, loss = 0.09449938\n",
      "Iteration 50, loss = 0.09364583\n",
      "Iteration 51, loss = 0.09191696\n",
      "Iteration 52, loss = 0.09429376\n",
      "Iteration 53, loss = 0.09422628\n",
      "Iteration 54, loss = 0.09221338\n",
      "Iteration 55, loss = 0.09203255\n",
      "Iteration 56, loss = 0.09102704\n",
      "Iteration 57, loss = 0.09171560\n",
      "Iteration 58, loss = 0.09122478\n",
      "Iteration 59, loss = 0.08990120\n",
      "Iteration 60, loss = 0.08865875\n",
      "Iteration 61, loss = 0.08881294\n",
      "Iteration 62, loss = 0.08879288\n",
      "Iteration 63, loss = 0.08860555\n",
      "Iteration 64, loss = 0.08777077\n",
      "Iteration 65, loss = 0.08787412\n",
      "Iteration 66, loss = 0.08708930\n",
      "Iteration 67, loss = 0.08779476\n",
      "Iteration 68, loss = 0.09092829\n",
      "Iteration 69, loss = 0.08514539\n",
      "Iteration 70, loss = 0.08689073\n",
      "Iteration 71, loss = 0.08591322\n",
      "Iteration 72, loss = 0.08710700\n",
      "Iteration 73, loss = 0.08976379\n",
      "Iteration 74, loss = 0.08605500\n",
      "Iteration 75, loss = 0.08771557\n",
      "Iteration 76, loss = 0.08465812\n",
      "Iteration 77, loss = 0.08647384\n",
      "Iteration 78, loss = 0.08628432\n",
      "Iteration 79, loss = 0.08615075\n",
      "Iteration 80, loss = 0.08530583\n",
      "Iteration 1, loss = 0.67866008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.63916327\n",
      "Iteration 3, loss = 0.59438628\n",
      "Iteration 4, loss = 0.54098325\n",
      "Iteration 5, loss = 0.48155852\n",
      "Iteration 6, loss = 0.42937754\n",
      "Iteration 7, loss = 0.36348808\n",
      "Iteration 8, loss = 0.31107206\n",
      "Iteration 9, loss = 0.27034777\n",
      "Iteration 10, loss = 0.24194458\n",
      "Iteration 11, loss = 0.21793406\n",
      "Iteration 12, loss = 0.20053561\n",
      "Iteration 13, loss = 0.18716561\n",
      "Iteration 14, loss = 0.17391701\n",
      "Iteration 15, loss = 0.16240162\n",
      "Iteration 16, loss = 0.15449054\n",
      "Iteration 17, loss = 0.14877229\n",
      "Iteration 18, loss = 0.14404307\n",
      "Iteration 19, loss = 0.14084939\n",
      "Iteration 20, loss = 0.13789864\n",
      "Iteration 21, loss = 0.13623995\n",
      "Iteration 22, loss = 0.13354960\n",
      "Iteration 23, loss = 0.13226681\n",
      "Iteration 24, loss = 0.13025673\n",
      "Iteration 25, loss = 0.12930643\n",
      "Iteration 26, loss = 0.12680163\n",
      "Iteration 27, loss = 0.12576313\n",
      "Iteration 28, loss = 0.12529404\n",
      "Iteration 29, loss = 0.12174059\n",
      "Iteration 30, loss = 0.12199998\n",
      "Iteration 31, loss = 0.12151127\n",
      "Iteration 32, loss = 0.11886404\n",
      "Iteration 33, loss = 0.12136265\n",
      "Iteration 34, loss = 0.11693410\n",
      "Iteration 35, loss = 0.11500546\n",
      "Iteration 36, loss = 0.11511264\n",
      "Iteration 37, loss = 0.11270880\n",
      "Iteration 38, loss = 0.11075910\n",
      "Iteration 39, loss = 0.11326721\n",
      "Iteration 40, loss = 0.11011155\n",
      "Iteration 41, loss = 0.10976105\n",
      "Iteration 42, loss = 0.11043421\n",
      "Iteration 43, loss = 0.10580563\n",
      "Iteration 44, loss = 0.10446698\n",
      "Iteration 45, loss = 0.10868524\n",
      "Iteration 46, loss = 0.10232371\n",
      "Iteration 47, loss = 0.10389780\n",
      "Iteration 48, loss = 0.10107138\n",
      "Iteration 49, loss = 0.10099150\n",
      "Iteration 50, loss = 0.09895471\n",
      "Iteration 51, loss = 0.09917404\n",
      "Iteration 52, loss = 0.10230020\n",
      "Iteration 53, loss = 0.09776602\n",
      "Iteration 54, loss = 0.09795978\n",
      "Iteration 55, loss = 0.10101469\n",
      "Iteration 56, loss = 0.09703086\n",
      "Iteration 57, loss = 0.09680226\n",
      "Iteration 58, loss = 0.09757765\n",
      "Iteration 59, loss = 0.09975307\n",
      "Iteration 60, loss = 0.09578857\n",
      "Iteration 61, loss = 0.09777736\n",
      "Iteration 62, loss = 0.09692281\n",
      "Iteration 63, loss = 0.09508102\n",
      "Iteration 64, loss = 0.09814110\n",
      "Iteration 65, loss = 0.09528734\n",
      "Iteration 66, loss = 0.09627033\n",
      "Iteration 67, loss = 0.09580286\n",
      "Iteration 68, loss = 0.09301739\n",
      "Iteration 69, loss = 0.09290870\n",
      "Iteration 70, loss = 0.09958838\n",
      "Iteration 71, loss = 0.09562785\n",
      "Iteration 72, loss = 0.09501598\n",
      "Iteration 73, loss = 0.09292740\n",
      "Iteration 74, loss = 0.09559941\n",
      "Iteration 75, loss = 0.09338616\n",
      "Iteration 76, loss = 0.09106619\n",
      "Iteration 77, loss = 0.09479740\n",
      "Iteration 78, loss = 0.09529755\n",
      "Iteration 79, loss = 0.09313188\n",
      "Iteration 80, loss = 0.09564688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.67209637\n",
      "Iteration 2, loss = 0.62065550\n",
      "Iteration 3, loss = 0.56087405\n",
      "Iteration 4, loss = 0.51794755\n",
      "Iteration 5, loss = 0.47196340\n",
      "Iteration 6, loss = 0.42750479\n",
      "Iteration 7, loss = 0.37671002\n",
      "Iteration 8, loss = 0.31832305\n",
      "Iteration 9, loss = 0.27999031\n",
      "Iteration 10, loss = 0.25317603\n",
      "Iteration 11, loss = 0.22973512\n",
      "Iteration 12, loss = 0.20824907\n",
      "Iteration 13, loss = 0.19090604\n",
      "Iteration 14, loss = 0.17605529\n",
      "Iteration 15, loss = 0.16186292\n",
      "Iteration 16, loss = 0.15106188\n",
      "Iteration 17, loss = 0.14289418\n",
      "Iteration 18, loss = 0.13709139\n",
      "Iteration 19, loss = 0.13202643\n",
      "Iteration 20, loss = 0.12861283\n",
      "Iteration 21, loss = 0.12452148\n",
      "Iteration 22, loss = 0.12186059\n",
      "Iteration 23, loss = 0.11795346\n",
      "Iteration 24, loss = 0.11381347\n",
      "Iteration 25, loss = 0.11024389\n",
      "Iteration 26, loss = 0.10669673\n",
      "Iteration 27, loss = 0.10359034\n",
      "Iteration 28, loss = 0.10237053\n",
      "Iteration 29, loss = 0.10028243\n",
      "Iteration 30, loss = 0.09894959\n",
      "Iteration 31, loss = 0.09872563\n",
      "Iteration 32, loss = 0.09815968\n",
      "Iteration 33, loss = 0.09754027\n",
      "Iteration 34, loss = 0.09506553\n",
      "Iteration 35, loss = 0.09300018\n",
      "Iteration 36, loss = 0.09140930\n",
      "Iteration 37, loss = 0.09076245\n",
      "Iteration 38, loss = 0.08950093\n",
      "Iteration 39, loss = 0.08874180\n",
      "Iteration 40, loss = 0.08932267\n",
      "Iteration 41, loss = 0.08836073\n",
      "Iteration 42, loss = 0.08622282\n",
      "Iteration 43, loss = 0.08632502\n",
      "Iteration 44, loss = 0.08518612\n",
      "Iteration 45, loss = 0.08389765\n",
      "Iteration 46, loss = 0.08367214\n",
      "Iteration 47, loss = 0.08097983\n",
      "Iteration 48, loss = 0.08355492\n",
      "Iteration 49, loss = 0.08009809\n",
      "Iteration 50, loss = 0.08199591\n",
      "Iteration 51, loss = 0.08060884\n",
      "Iteration 52, loss = 0.07850945\n",
      "Iteration 53, loss = 0.08399036\n",
      "Iteration 54, loss = 0.07810830\n",
      "Iteration 55, loss = 0.07726358\n",
      "Iteration 56, loss = 0.07636380\n",
      "Iteration 57, loss = 0.07638389\n",
      "Iteration 58, loss = 0.07781728\n",
      "Iteration 59, loss = 0.07465281\n",
      "Iteration 60, loss = 0.07495049\n",
      "Iteration 61, loss = 0.07449177\n",
      "Iteration 62, loss = 0.07404262\n",
      "Iteration 63, loss = 0.07424443\n",
      "Iteration 64, loss = 0.07783868\n",
      "Iteration 65, loss = 0.07327422\n",
      "Iteration 66, loss = 0.07434572\n",
      "Iteration 67, loss = 0.07263314\n",
      "Iteration 68, loss = 0.07335482\n",
      "Iteration 69, loss = 0.07311272\n",
      "Iteration 70, loss = 0.07142557\n",
      "Iteration 71, loss = 0.07220475\n",
      "Iteration 72, loss = 0.07092405\n",
      "Iteration 73, loss = 0.07347096\n",
      "Iteration 74, loss = 0.07234105\n",
      "Iteration 75, loss = 0.06987090\n",
      "Iteration 76, loss = 0.06928677\n",
      "Iteration 77, loss = 0.07143199\n",
      "Iteration 78, loss = 0.06995125\n",
      "Iteration 79, loss = 0.07072233\n",
      "Iteration 80, loss = 0.07010963\n",
      "Iteration 1, loss = 0.68441897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.61023862\n",
      "Iteration 3, loss = 0.52890261\n",
      "Iteration 4, loss = 0.47727719\n",
      "Iteration 5, loss = 0.44565959\n",
      "Iteration 6, loss = 0.42536945\n",
      "Iteration 7, loss = 0.40924664\n",
      "Iteration 8, loss = 0.39488469\n",
      "Iteration 9, loss = 0.37893473\n",
      "Iteration 10, loss = 0.36348710\n",
      "Iteration 11, loss = 0.35011331\n",
      "Iteration 12, loss = 0.33900486\n",
      "Iteration 13, loss = 0.32180694\n",
      "Iteration 14, loss = 0.31085820\n",
      "Iteration 15, loss = 0.29837048\n",
      "Iteration 16, loss = 0.28489615\n",
      "Iteration 17, loss = 0.26106024\n",
      "Iteration 18, loss = 0.23816927\n",
      "Iteration 19, loss = 0.22750744\n",
      "Iteration 20, loss = 0.21839326\n",
      "Iteration 21, loss = 0.20965837\n",
      "Iteration 22, loss = 0.20153913\n",
      "Iteration 23, loss = 0.19417917\n",
      "Iteration 24, loss = 0.18963468\n",
      "Iteration 25, loss = 0.18213347\n",
      "Iteration 26, loss = 0.17785164\n",
      "Iteration 27, loss = 0.16987875\n",
      "Iteration 28, loss = 0.16752213\n",
      "Iteration 29, loss = 0.16493951\n",
      "Iteration 30, loss = 0.15565397\n",
      "Iteration 31, loss = 0.15266211\n",
      "Iteration 32, loss = 0.14719188\n",
      "Iteration 33, loss = 0.14677164\n",
      "Iteration 34, loss = 0.14597017\n",
      "Iteration 35, loss = 0.13879777\n",
      "Iteration 36, loss = 0.13809490\n",
      "Iteration 37, loss = 0.13603526\n",
      "Iteration 38, loss = 0.13646167\n",
      "Iteration 39, loss = 0.13318828\n",
      "Iteration 40, loss = 0.12959300\n",
      "Iteration 41, loss = 0.12758585\n",
      "Iteration 42, loss = 0.12992511\n",
      "Iteration 43, loss = 0.12582011\n",
      "Iteration 44, loss = 0.13280851\n",
      "Iteration 45, loss = 0.12812940\n",
      "Iteration 46, loss = 0.12382205\n",
      "Iteration 47, loss = 0.12486878\n",
      "Iteration 48, loss = 0.12303164\n",
      "Iteration 49, loss = 0.12091584\n",
      "Iteration 50, loss = 0.11934031\n",
      "Iteration 51, loss = 0.12101980\n",
      "Iteration 52, loss = 0.12205754\n",
      "Iteration 53, loss = 0.11930271\n",
      "Iteration 54, loss = 0.11891307\n",
      "Iteration 55, loss = 0.11592184\n",
      "Iteration 56, loss = 0.11826954\n",
      "Iteration 57, loss = 0.11467404\n",
      "Iteration 58, loss = 0.11435390\n",
      "Iteration 59, loss = 0.11462796\n",
      "Iteration 60, loss = 0.11514286\n",
      "Iteration 61, loss = 0.11301525\n",
      "Iteration 62, loss = 0.11322213\n",
      "Iteration 63, loss = 0.11186451\n",
      "Iteration 64, loss = 0.11225492\n",
      "Iteration 65, loss = 0.10788632\n",
      "Iteration 66, loss = 0.10776696\n",
      "Iteration 67, loss = 0.10780729\n",
      "Iteration 68, loss = 0.10853644\n",
      "Iteration 69, loss = 0.11006568\n",
      "Iteration 70, loss = 0.11016296\n",
      "Iteration 71, loss = 0.10701547\n",
      "Iteration 72, loss = 0.10446850\n",
      "Iteration 73, loss = 0.10933323\n",
      "Iteration 74, loss = 0.11368128\n",
      "Iteration 75, loss = 0.10618649\n",
      "Iteration 76, loss = 0.10882090\n",
      "Iteration 77, loss = 0.10704036\n",
      "Iteration 78, loss = 0.10750549\n",
      "Iteration 79, loss = 0.10991963\n",
      "Iteration 80, loss = 0.10416028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63912879\n",
      "Iteration 2, loss = 0.56048922\n",
      "Iteration 3, loss = 0.52035108\n",
      "Iteration 4, loss = 0.47768008\n",
      "Iteration 5, loss = 0.42263797\n",
      "Iteration 6, loss = 0.36903364\n",
      "Iteration 7, loss = 0.33028511\n",
      "Iteration 8, loss = 0.30284686\n",
      "Iteration 9, loss = 0.28180957\n",
      "Iteration 10, loss = 0.26856204\n",
      "Iteration 11, loss = 0.25742643\n",
      "Iteration 12, loss = 0.24986917\n",
      "Iteration 13, loss = 0.24246912\n",
      "Iteration 14, loss = 0.23508477\n",
      "Iteration 15, loss = 0.22828203\n",
      "Iteration 16, loss = 0.22348241\n",
      "Iteration 17, loss = 0.21781873\n",
      "Iteration 18, loss = 0.21082462\n",
      "Iteration 19, loss = 0.20649293\n",
      "Iteration 20, loss = 0.20018075\n",
      "Iteration 21, loss = 0.19787714\n",
      "Iteration 22, loss = 0.19553102\n",
      "Iteration 23, loss = 0.19259145\n",
      "Iteration 24, loss = 0.18588615\n",
      "Iteration 25, loss = 0.18295224\n",
      "Iteration 26, loss = 0.18125147\n",
      "Iteration 27, loss = 0.18102019\n",
      "Iteration 28, loss = 0.17553954\n",
      "Iteration 29, loss = 0.17486611\n",
      "Iteration 30, loss = 0.17558595\n",
      "Iteration 31, loss = 0.17160293\n",
      "Iteration 32, loss = 0.17071686\n",
      "Iteration 33, loss = 0.17055588\n",
      "Iteration 34, loss = 0.16522193\n",
      "Iteration 35, loss = 0.16195870\n",
      "Iteration 36, loss = 0.16315499\n",
      "Iteration 37, loss = 0.15901721\n",
      "Iteration 38, loss = 0.16057461\n",
      "Iteration 39, loss = 0.15765369\n",
      "Iteration 40, loss = 0.15860736\n",
      "Iteration 41, loss = 0.15567357\n",
      "Iteration 42, loss = 0.15196755\n",
      "Iteration 43, loss = 0.15368168\n",
      "Iteration 44, loss = 0.15246382\n",
      "Iteration 45, loss = 0.15352781\n",
      "Iteration 46, loss = 0.15117714\n",
      "Iteration 47, loss = 0.15285288\n",
      "Iteration 48, loss = 0.15209991\n",
      "Iteration 49, loss = 0.14948434\n",
      "Iteration 50, loss = 0.14864101\n",
      "Iteration 51, loss = 0.14844546\n",
      "Iteration 52, loss = 0.14892834\n",
      "Iteration 53, loss = 0.14861267\n",
      "Iteration 54, loss = 0.14865093\n",
      "Iteration 55, loss = 0.14624213\n",
      "Iteration 56, loss = 0.14619080\n",
      "Iteration 57, loss = 0.14801053\n",
      "Iteration 58, loss = 0.15127615\n",
      "Iteration 59, loss = 0.14547441\n",
      "Iteration 60, loss = 0.14438875\n",
      "Iteration 61, loss = 0.14543717\n",
      "Iteration 62, loss = 0.14370337\n",
      "Iteration 63, loss = 0.14454029\n",
      "Iteration 64, loss = 0.14277409\n",
      "Iteration 65, loss = 0.14275806\n",
      "Iteration 66, loss = 0.14517942\n",
      "Iteration 67, loss = 0.14333009\n",
      "Iteration 68, loss = 0.14226374\n",
      "Iteration 69, loss = 0.14371466\n",
      "Iteration 70, loss = 0.14016108\n",
      "Iteration 71, loss = 0.14166463\n",
      "Iteration 72, loss = 0.14048199\n",
      "Iteration 73, loss = 0.14120390\n",
      "Iteration 74, loss = 0.14243252\n",
      "Iteration 75, loss = 0.13957934\n",
      "Iteration 76, loss = 0.14502449\n",
      "Iteration 77, loss = 0.13788298\n",
      "Iteration 78, loss = 0.14173510\n",
      "Iteration 79, loss = 0.13890823\n",
      "Iteration 80, loss = 0.13975027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.64845818\n",
      "Iteration 2, loss = 0.52769463\n",
      "Iteration 3, loss = 0.42718864\n",
      "Iteration 4, loss = 0.34393920\n",
      "Iteration 5, loss = 0.29448693\n",
      "Iteration 6, loss = 0.26766558\n",
      "Iteration 7, loss = 0.25244228\n",
      "Iteration 8, loss = 0.23905669\n",
      "Iteration 9, loss = 0.22489916\n",
      "Iteration 10, loss = 0.21016925\n",
      "Iteration 11, loss = 0.19729771\n",
      "Iteration 12, loss = 0.18719064\n",
      "Iteration 13, loss = 0.17590286\n",
      "Iteration 14, loss = 0.16611484\n",
      "Iteration 15, loss = 0.15639424\n",
      "Iteration 16, loss = 0.14736825\n",
      "Iteration 17, loss = 0.13847689\n",
      "Iteration 18, loss = 0.13015816\n",
      "Iteration 19, loss = 0.12363842\n",
      "Iteration 20, loss = 0.11686738\n",
      "Iteration 21, loss = 0.11099529\n",
      "Iteration 22, loss = 0.10713243\n",
      "Iteration 23, loss = 0.10231046\n",
      "Iteration 24, loss = 0.09851722\n",
      "Iteration 25, loss = 0.09435349\n",
      "Iteration 26, loss = 0.09202514\n",
      "Iteration 27, loss = 0.08849483\n",
      "Iteration 28, loss = 0.08724865\n",
      "Iteration 29, loss = 0.08515983\n",
      "Iteration 30, loss = 0.08262853\n",
      "Iteration 31, loss = 0.08225697\n",
      "Iteration 32, loss = 0.08224878\n",
      "Iteration 33, loss = 0.07986439\n",
      "Iteration 34, loss = 0.08164231\n",
      "Iteration 35, loss = 0.07804498\n",
      "Iteration 36, loss = 0.08069358\n",
      "Iteration 37, loss = 0.07515309\n",
      "Iteration 38, loss = 0.07524914\n",
      "Iteration 39, loss = 0.07528503\n",
      "Iteration 40, loss = 0.07817115\n",
      "Iteration 41, loss = 0.07645574\n",
      "Iteration 42, loss = 0.07583743\n",
      "Iteration 43, loss = 0.07355224\n",
      "Iteration 44, loss = 0.07483180\n",
      "Iteration 45, loss = 0.07369204\n",
      "Iteration 46, loss = 0.07109426\n",
      "Iteration 47, loss = 0.07568316\n",
      "Iteration 48, loss = 0.07182864\n",
      "Iteration 49, loss = 0.07114699\n",
      "Iteration 50, loss = 0.06967937\n",
      "Iteration 51, loss = 0.07292792\n",
      "Iteration 52, loss = 0.07570115\n",
      "Iteration 53, loss = 0.07135269\n",
      "Iteration 54, loss = 0.07558064\n",
      "Iteration 55, loss = 0.07239642\n",
      "Iteration 56, loss = 0.07034191\n",
      "Iteration 57, loss = 0.06820717\n",
      "Iteration 58, loss = 0.06893161\n",
      "Iteration 59, loss = 0.07239080\n",
      "Iteration 60, loss = 0.06725202\n",
      "Iteration 61, loss = 0.07213223\n",
      "Iteration 62, loss = 0.07318174\n",
      "Iteration 63, loss = 0.06936314\n",
      "Iteration 64, loss = 0.07002303\n",
      "Iteration 65, loss = 0.06757561\n",
      "Iteration 66, loss = 0.07082868\n",
      "Iteration 67, loss = 0.06767856\n",
      "Iteration 68, loss = 0.06802232\n",
      "Iteration 69, loss = 0.06837794\n",
      "Iteration 70, loss = 0.07292293\n",
      "Iteration 71, loss = 0.06801807\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.73328575\n",
      "Iteration 2, loss = 0.60643277\n",
      "Iteration 3, loss = 0.49168325\n",
      "Iteration 4, loss = 0.35451804\n",
      "Iteration 5, loss = 0.24324013\n",
      "Iteration 6, loss = 0.18512023\n",
      "Iteration 7, loss = 0.15762320\n",
      "Iteration 8, loss = 0.14091769\n",
      "Iteration 9, loss = 0.12758177\n",
      "Iteration 10, loss = 0.11821805\n",
      "Iteration 11, loss = 0.11204698\n",
      "Iteration 12, loss = 0.10777010\n",
      "Iteration 13, loss = 0.10318884\n",
      "Iteration 14, loss = 0.10088810\n",
      "Iteration 15, loss = 0.09642324\n",
      "Iteration 16, loss = 0.09425665\n",
      "Iteration 17, loss = 0.09249036\n",
      "Iteration 18, loss = 0.09156610\n",
      "Iteration 19, loss = 0.08936297\n",
      "Iteration 20, loss = 0.08945313\n",
      "Iteration 21, loss = 0.08681447\n",
      "Iteration 22, loss = 0.08719161\n",
      "Iteration 23, loss = 0.08563684\n",
      "Iteration 24, loss = 0.08504156\n",
      "Iteration 25, loss = 0.08431136\n",
      "Iteration 26, loss = 0.08472570\n",
      "Iteration 27, loss = 0.08418885\n",
      "Iteration 28, loss = 0.08395960\n",
      "Iteration 29, loss = 0.08159860\n",
      "Iteration 30, loss = 0.08240523\n",
      "Iteration 31, loss = 0.08129795\n",
      "Iteration 32, loss = 0.08191305\n",
      "Iteration 33, loss = 0.08060581\n",
      "Iteration 34, loss = 0.08174268\n",
      "Iteration 35, loss = 0.08159489\n",
      "Iteration 36, loss = 0.08213887\n",
      "Iteration 37, loss = 0.07922931\n",
      "Iteration 38, loss = 0.07901173\n",
      "Iteration 39, loss = 0.07859534\n",
      "Iteration 40, loss = 0.07837274\n",
      "Iteration 41, loss = 0.07955924\n",
      "Iteration 42, loss = 0.07696707\n",
      "Iteration 43, loss = 0.07798569\n",
      "Iteration 44, loss = 0.07627885\n",
      "Iteration 45, loss = 0.07785144\n",
      "Iteration 46, loss = 0.07650226\n",
      "Iteration 47, loss = 0.07786451\n",
      "Iteration 48, loss = 0.07698474\n",
      "Iteration 49, loss = 0.07673338\n",
      "Iteration 50, loss = 0.07628943\n",
      "Iteration 51, loss = 0.07931014\n",
      "Iteration 52, loss = 0.07433689\n",
      "Iteration 53, loss = 0.07723544\n",
      "Iteration 54, loss = 0.07458992\n",
      "Iteration 55, loss = 0.07360492\n",
      "Iteration 56, loss = 0.07366384\n",
      "Iteration 57, loss = 0.07504831\n",
      "Iteration 58, loss = 0.07400456\n",
      "Iteration 59, loss = 0.07455620\n",
      "Iteration 60, loss = 0.07459484\n",
      "Iteration 61, loss = 0.07568881\n",
      "Iteration 62, loss = 0.07187591\n",
      "Iteration 63, loss = 0.07247798\n",
      "Iteration 64, loss = 0.07325712\n",
      "Iteration 65, loss = 0.07529960\n",
      "Iteration 66, loss = 0.07277635\n",
      "Iteration 67, loss = 0.07166977\n",
      "Iteration 68, loss = 0.07122827\n",
      "Iteration 69, loss = 0.07355260\n",
      "Iteration 70, loss = 0.07207711\n",
      "Iteration 71, loss = 0.07159616\n",
      "Iteration 72, loss = 0.07045358\n",
      "Iteration 73, loss = 0.07237833\n",
      "Iteration 74, loss = 0.07089524\n",
      "Iteration 75, loss = 0.07188086\n",
      "Iteration 76, loss = 0.07006457\n",
      "Iteration 77, loss = 0.07318554\n",
      "Iteration 78, loss = 0.07197013\n",
      "Iteration 79, loss = 0.07226597\n",
      "Iteration 80, loss = 0.07106732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69312615\n",
      "Iteration 2, loss = 0.62804962\n",
      "Iteration 3, loss = 0.57235221\n",
      "Iteration 4, loss = 0.51198469\n",
      "Iteration 5, loss = 0.46747848\n",
      "Iteration 6, loss = 0.42792084\n",
      "Iteration 7, loss = 0.38920134\n",
      "Iteration 8, loss = 0.34966880\n",
      "Iteration 9, loss = 0.30746757\n",
      "Iteration 10, loss = 0.28211250\n",
      "Iteration 11, loss = 0.26099852\n",
      "Iteration 12, loss = 0.23765955\n",
      "Iteration 13, loss = 0.22113101\n",
      "Iteration 14, loss = 0.20988986\n",
      "Iteration 15, loss = 0.20125876\n",
      "Iteration 16, loss = 0.18989512\n",
      "Iteration 17, loss = 0.18616556\n",
      "Iteration 18, loss = 0.18078527\n",
      "Iteration 19, loss = 0.17842207\n",
      "Iteration 20, loss = 0.17176153\n",
      "Iteration 21, loss = 0.16647065\n",
      "Iteration 22, loss = 0.16353185\n",
      "Iteration 23, loss = 0.16084395\n",
      "Iteration 24, loss = 0.15799721\n",
      "Iteration 25, loss = 0.15414834\n",
      "Iteration 26, loss = 0.15192583\n",
      "Iteration 27, loss = 0.14788900\n",
      "Iteration 28, loss = 0.14653555\n",
      "Iteration 29, loss = 0.14323919\n",
      "Iteration 30, loss = 0.13853781\n",
      "Iteration 31, loss = 0.13948704\n",
      "Iteration 32, loss = 0.13878408\n",
      "Iteration 33, loss = 0.13617540\n",
      "Iteration 34, loss = 0.13102261\n",
      "Iteration 35, loss = 0.12685054\n",
      "Iteration 36, loss = 0.12490029\n",
      "Iteration 37, loss = 0.12233859\n",
      "Iteration 38, loss = 0.11954603\n",
      "Iteration 39, loss = 0.12286645\n",
      "Iteration 40, loss = 0.11760923\n",
      "Iteration 41, loss = 0.11473686\n",
      "Iteration 42, loss = 0.11512255\n",
      "Iteration 43, loss = 0.11137587\n",
      "Iteration 44, loss = 0.11239347\n",
      "Iteration 45, loss = 0.10727043\n",
      "Iteration 46, loss = 0.11056787\n",
      "Iteration 47, loss = 0.11065031\n",
      "Iteration 48, loss = 0.10704961\n",
      "Iteration 49, loss = 0.10320156\n",
      "Iteration 50, loss = 0.10490477\n",
      "Iteration 51, loss = 0.10315384\n",
      "Iteration 52, loss = 0.10310506\n",
      "Iteration 53, loss = 0.09914259\n",
      "Iteration 54, loss = 0.10191297\n",
      "Iteration 55, loss = 0.09907139\n",
      "Iteration 56, loss = 0.10077016\n",
      "Iteration 57, loss = 0.09538040\n",
      "Iteration 58, loss = 0.10160177\n",
      "Iteration 59, loss = 0.09456670\n",
      "Iteration 60, loss = 0.09321143\n",
      "Iteration 61, loss = 0.09358821\n",
      "Iteration 62, loss = 0.09273882\n",
      "Iteration 63, loss = 0.09392272\n",
      "Iteration 64, loss = 0.09221418\n",
      "Iteration 65, loss = 0.08898655\n",
      "Iteration 66, loss = 0.08995992\n",
      "Iteration 67, loss = 0.09083070\n",
      "Iteration 68, loss = 0.08716256\n",
      "Iteration 69, loss = 0.09162434\n",
      "Iteration 70, loss = 0.08949388\n",
      "Iteration 71, loss = 0.09197133\n",
      "Iteration 72, loss = 0.08873206\n",
      "Iteration 73, loss = 0.08564685\n",
      "Iteration 74, loss = 0.08578871\n",
      "Iteration 75, loss = 0.08952070\n",
      "Iteration 76, loss = 0.08711644\n",
      "Iteration 77, loss = 0.08548553\n",
      "Iteration 78, loss = 0.08888063\n",
      "Iteration 79, loss = 0.08632191\n",
      "Iteration 80, loss = 0.08614811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69884758\n",
      "Iteration 2, loss = 0.62844543\n",
      "Iteration 3, loss = 0.58019359\n",
      "Iteration 4, loss = 0.52684021\n",
      "Iteration 5, loss = 0.45997843\n",
      "Iteration 6, loss = 0.40244210\n",
      "Iteration 7, loss = 0.35409121\n",
      "Iteration 8, loss = 0.31020759\n",
      "Iteration 9, loss = 0.27696329\n",
      "Iteration 10, loss = 0.25529790\n",
      "Iteration 11, loss = 0.23832257\n",
      "Iteration 12, loss = 0.22165312\n",
      "Iteration 13, loss = 0.21475769\n",
      "Iteration 14, loss = 0.20210755\n",
      "Iteration 15, loss = 0.19548108\n",
      "Iteration 16, loss = 0.18754592\n",
      "Iteration 17, loss = 0.17887531\n",
      "Iteration 18, loss = 0.17292225\n",
      "Iteration 19, loss = 0.16553552\n",
      "Iteration 20, loss = 0.15963623\n",
      "Iteration 21, loss = 0.15453070\n",
      "Iteration 22, loss = 0.14828820\n",
      "Iteration 23, loss = 0.14512400\n",
      "Iteration 24, loss = 0.14145973\n",
      "Iteration 25, loss = 0.13656597\n",
      "Iteration 26, loss = 0.13842270\n",
      "Iteration 27, loss = 0.13212123\n",
      "Iteration 28, loss = 0.12942818\n",
      "Iteration 29, loss = 0.13094455\n",
      "Iteration 30, loss = 0.12374005\n",
      "Iteration 31, loss = 0.12348066\n",
      "Iteration 32, loss = 0.12171976\n",
      "Iteration 33, loss = 0.11806526\n",
      "Iteration 34, loss = 0.12103926\n",
      "Iteration 35, loss = 0.11511688\n",
      "Iteration 36, loss = 0.11565561\n",
      "Iteration 37, loss = 0.11209088\n",
      "Iteration 38, loss = 0.11535686\n",
      "Iteration 39, loss = 0.11033038\n",
      "Iteration 40, loss = 0.11072223\n",
      "Iteration 41, loss = 0.10883190\n",
      "Iteration 42, loss = 0.10221769\n",
      "Iteration 43, loss = 0.10751423\n",
      "Iteration 44, loss = 0.10530918\n",
      "Iteration 45, loss = 0.10551658\n",
      "Iteration 46, loss = 0.10570139\n",
      "Iteration 47, loss = 0.10341290\n",
      "Iteration 48, loss = 0.10167230\n",
      "Iteration 49, loss = 0.10226300\n",
      "Iteration 50, loss = 0.09994468\n",
      "Iteration 51, loss = 0.10322866\n",
      "Iteration 52, loss = 0.10272466\n",
      "Iteration 53, loss = 0.09664405\n",
      "Iteration 54, loss = 0.09690848\n",
      "Iteration 55, loss = 0.09907721\n",
      "Iteration 56, loss = 0.09871459\n",
      "Iteration 57, loss = 0.09594385\n",
      "Iteration 58, loss = 0.09985839\n",
      "Iteration 59, loss = 0.09828846\n",
      "Iteration 60, loss = 0.09762096\n",
      "Iteration 61, loss = 0.09938827\n",
      "Iteration 62, loss = 0.09509103\n",
      "Iteration 63, loss = 0.09775491\n",
      "Iteration 64, loss = 0.09151069\n",
      "Iteration 65, loss = 0.09276692\n",
      "Iteration 66, loss = 0.09515870\n",
      "Iteration 67, loss = 0.09935189\n",
      "Iteration 68, loss = 0.09557271\n",
      "Iteration 69, loss = 0.09301533\n",
      "Iteration 70, loss = 0.09504553\n",
      "Iteration 71, loss = 0.09441498\n",
      "Iteration 72, loss = 0.09221833\n",
      "Iteration 73, loss = 0.09291434\n",
      "Iteration 74, loss = 0.08790096\n",
      "Iteration 75, loss = 0.08919241\n",
      "Iteration 76, loss = 0.08669138\n",
      "Iteration 77, loss = 0.09099040\n",
      "Iteration 78, loss = 0.09105489\n",
      "Iteration 79, loss = 0.08823846\n",
      "Iteration 80, loss = 0.09423769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.73755471\n",
      "Iteration 2, loss = 0.66959802\n",
      "Iteration 3, loss = 0.64307481\n",
      "Iteration 4, loss = 0.61254808\n",
      "Iteration 5, loss = 0.58790395\n",
      "Iteration 6, loss = 0.54474595\n",
      "Iteration 7, loss = 0.50325863\n",
      "Iteration 8, loss = 0.47532584\n",
      "Iteration 9, loss = 0.46157481\n",
      "Iteration 10, loss = 0.44927881\n",
      "Iteration 11, loss = 0.43989477\n",
      "Iteration 12, loss = 0.42536647\n",
      "Iteration 13, loss = 0.41026957\n",
      "Iteration 14, loss = 0.39145245\n",
      "Iteration 15, loss = 0.36594977\n",
      "Iteration 16, loss = 0.33331520\n",
      "Iteration 17, loss = 0.31336759\n",
      "Iteration 18, loss = 0.30171841\n",
      "Iteration 19, loss = 0.28807129\n",
      "Iteration 20, loss = 0.27845666\n",
      "Iteration 21, loss = 0.27275388\n",
      "Iteration 22, loss = 0.26377730\n",
      "Iteration 23, loss = 0.25633959\n",
      "Iteration 24, loss = 0.25013288\n",
      "Iteration 25, loss = 0.24498021\n",
      "Iteration 26, loss = 0.23875291\n",
      "Iteration 27, loss = 0.23662104\n",
      "Iteration 28, loss = 0.23488258\n",
      "Iteration 29, loss = 0.23429673\n",
      "Iteration 30, loss = 0.22859413\n",
      "Iteration 31, loss = 0.22412089\n",
      "Iteration 32, loss = 0.22080980\n",
      "Iteration 33, loss = 0.21846978\n",
      "Iteration 34, loss = 0.21761936\n",
      "Iteration 35, loss = 0.21440516\n",
      "Iteration 36, loss = 0.21169970\n",
      "Iteration 37, loss = 0.20557978\n",
      "Iteration 38, loss = 0.20441321\n",
      "Iteration 39, loss = 0.20347941\n",
      "Iteration 40, loss = 0.20243862\n",
      "Iteration 41, loss = 0.20329987\n",
      "Iteration 42, loss = 0.20010416\n",
      "Iteration 43, loss = 0.19899684\n",
      "Iteration 44, loss = 0.19860216\n",
      "Iteration 45, loss = 0.19572828\n",
      "Iteration 46, loss = 0.19681008\n",
      "Iteration 47, loss = 0.19519404\n",
      "Iteration 48, loss = 0.19565491\n",
      "Iteration 49, loss = 0.19285798\n",
      "Iteration 50, loss = 0.19010100\n",
      "Iteration 51, loss = 0.19291603\n",
      "Iteration 52, loss = 0.18833539\n",
      "Iteration 53, loss = 0.18709686\n",
      "Iteration 54, loss = 0.18588614\n",
      "Iteration 55, loss = 0.18968985\n",
      "Iteration 56, loss = 0.18310499\n",
      "Iteration 57, loss = 0.18264688\n",
      "Iteration 58, loss = 0.18337266\n",
      "Iteration 59, loss = 0.18186574\n",
      "Iteration 60, loss = 0.17850857\n",
      "Iteration 61, loss = 0.18108885\n",
      "Iteration 62, loss = 0.18536045\n",
      "Iteration 63, loss = 0.17616177\n",
      "Iteration 64, loss = 0.17493992\n",
      "Iteration 65, loss = 0.17722658\n",
      "Iteration 66, loss = 0.17407352\n",
      "Iteration 67, loss = 0.17186987\n",
      "Iteration 68, loss = 0.17283034\n",
      "Iteration 69, loss = 0.17629894\n",
      "Iteration 70, loss = 0.17347748\n",
      "Iteration 71, loss = 0.16846228\n",
      "Iteration 72, loss = 0.16745639\n",
      "Iteration 73, loss = 0.16829416\n",
      "Iteration 74, loss = 0.16427081\n",
      "Iteration 75, loss = 0.16729884\n",
      "Iteration 76, loss = 0.16513126\n",
      "Iteration 77, loss = 0.16366537\n",
      "Iteration 78, loss = 0.16406212\n",
      "Iteration 79, loss = 0.15897128\n",
      "Iteration 80, loss = 0.16318162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.66928384\n",
      "Iteration 2, loss = 0.57067748\n",
      "Iteration 3, loss = 0.49152155\n",
      "Iteration 4, loss = 0.39964693\n",
      "Iteration 5, loss = 0.31647074\n",
      "Iteration 6, loss = 0.27523175\n",
      "Iteration 7, loss = 0.25412460\n",
      "Iteration 8, loss = 0.24312067\n",
      "Iteration 9, loss = 0.23358097\n",
      "Iteration 10, loss = 0.22859558\n",
      "Iteration 11, loss = 0.22423453\n",
      "Iteration 12, loss = 0.22043111\n",
      "Iteration 13, loss = 0.21907597\n",
      "Iteration 14, loss = 0.21642719\n",
      "Iteration 15, loss = 0.21311045\n",
      "Iteration 16, loss = 0.21168316\n",
      "Iteration 17, loss = 0.20820526\n",
      "Iteration 18, loss = 0.20579227\n",
      "Iteration 19, loss = 0.20333959\n",
      "Iteration 20, loss = 0.19985938\n",
      "Iteration 21, loss = 0.19679545\n",
      "Iteration 22, loss = 0.19472862\n",
      "Iteration 23, loss = 0.19174844\n",
      "Iteration 24, loss = 0.18840188\n",
      "Iteration 25, loss = 0.18589916\n",
      "Iteration 26, loss = 0.18156683\n",
      "Iteration 27, loss = 0.17767663\n",
      "Iteration 28, loss = 0.17493126\n",
      "Iteration 29, loss = 0.17234163\n",
      "Iteration 30, loss = 0.17220143\n",
      "Iteration 31, loss = 0.16901536\n",
      "Iteration 32, loss = 0.16635435\n",
      "Iteration 33, loss = 0.16402106\n",
      "Iteration 34, loss = 0.16122339\n",
      "Iteration 35, loss = 0.15859517\n",
      "Iteration 36, loss = 0.15569595\n",
      "Iteration 37, loss = 0.15133073\n",
      "Iteration 38, loss = 0.14918283\n",
      "Iteration 39, loss = 0.14755461\n",
      "Iteration 40, loss = 0.14225442\n",
      "Iteration 41, loss = 0.13737754\n",
      "Iteration 42, loss = 0.13050160\n",
      "Iteration 43, loss = 0.12682382\n",
      "Iteration 44, loss = 0.12290047\n",
      "Iteration 45, loss = 0.12156356\n",
      "Iteration 46, loss = 0.11749062\n",
      "Iteration 47, loss = 0.11483570\n",
      "Iteration 48, loss = 0.11303723\n",
      "Iteration 49, loss = 0.11003003\n",
      "Iteration 50, loss = 0.11009428\n",
      "Iteration 51, loss = 0.10856533\n",
      "Iteration 52, loss = 0.10654372\n",
      "Iteration 53, loss = 0.10471754\n",
      "Iteration 54, loss = 0.10404734\n",
      "Iteration 55, loss = 0.10353290\n",
      "Iteration 56, loss = 0.10221855\n",
      "Iteration 57, loss = 0.10351592\n",
      "Iteration 58, loss = 0.10029106\n",
      "Iteration 59, loss = 0.10195945\n",
      "Iteration 60, loss = 0.09710300\n",
      "Iteration 61, loss = 0.09739446\n",
      "Iteration 62, loss = 0.09681338\n",
      "Iteration 63, loss = 0.09712988\n",
      "Iteration 64, loss = 0.09943192\n",
      "Iteration 65, loss = 0.09578596\n",
      "Iteration 66, loss = 0.09574674\n",
      "Iteration 67, loss = 0.09355355\n",
      "Iteration 68, loss = 0.09494440\n",
      "Iteration 69, loss = 0.09348977\n",
      "Iteration 70, loss = 0.09120368\n",
      "Iteration 71, loss = 0.09211298\n",
      "Iteration 72, loss = 0.09269740\n",
      "Iteration 73, loss = 0.09176020\n",
      "Iteration 74, loss = 0.09243863\n",
      "Iteration 75, loss = 0.08897436\n",
      "Iteration 76, loss = 0.09229163\n",
      "Iteration 77, loss = 0.09186741\n",
      "Iteration 78, loss = 0.08973384\n",
      "Iteration 79, loss = 0.09024901\n",
      "Iteration 80, loss = 0.08985388\n",
      "0.9711388221153845\n",
      "{'hidden_layer_sizes': (5, 15, 15, 5)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\python\\nir\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {\"hidden_layer_sizes\":[(4,7,4),(6,12,6),(8,8,8,8),(5,15,15,5)] }\n",
    "grid = GridSearchCV(estimator=clf, param_grid=params, scoring = 'accuracy')\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccf6614f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_hidden_layer_sizes</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.821960</td>\n",
       "      <td>0.643912</td>\n",
       "      <td>0.003928</td>\n",
       "      <td>0.006053</td>\n",
       "      <td>(4, 7, 4)</td>\n",
       "      <td>{'hidden_layer_sizes': (4, 7, 4)}</td>\n",
       "      <td>0.927809</td>\n",
       "      <td>0.897611</td>\n",
       "      <td>0.770282</td>\n",
       "      <td>0.824594</td>\n",
       "      <td>0.911508</td>\n",
       "      <td>0.866361</td>\n",
       "      <td>0.059609</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.527428</td>\n",
       "      <td>0.845753</td>\n",
       "      <td>0.012635</td>\n",
       "      <td>0.006323</td>\n",
       "      <td>(6, 12, 6)</td>\n",
       "      <td>{'hidden_layer_sizes': (6, 12, 6)}</td>\n",
       "      <td>0.976187</td>\n",
       "      <td>0.968525</td>\n",
       "      <td>0.963642</td>\n",
       "      <td>0.943434</td>\n",
       "      <td>0.968299</td>\n",
       "      <td>0.964017</td>\n",
       "      <td>0.011050</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.899685</td>\n",
       "      <td>0.807141</td>\n",
       "      <td>0.010755</td>\n",
       "      <td>0.006318</td>\n",
       "      <td>(8, 8, 8, 8)</td>\n",
       "      <td>{'hidden_layer_sizes': (8, 8, 8, 8)}</td>\n",
       "      <td>0.977539</td>\n",
       "      <td>0.971680</td>\n",
       "      <td>0.983023</td>\n",
       "      <td>0.968975</td>\n",
       "      <td>0.946289</td>\n",
       "      <td>0.969501</td>\n",
       "      <td>0.012580</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.358911</td>\n",
       "      <td>0.778196</td>\n",
       "      <td>0.012321</td>\n",
       "      <td>0.007329</td>\n",
       "      <td>(5, 15, 15, 5)</td>\n",
       "      <td>{'hidden_layer_sizes': (5, 15, 15, 5)}</td>\n",
       "      <td>0.982197</td>\n",
       "      <td>0.981295</td>\n",
       "      <td>0.981746</td>\n",
       "      <td>0.975436</td>\n",
       "      <td>0.935021</td>\n",
       "      <td>0.971139</td>\n",
       "      <td>0.018226</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      11.821960      0.643912         0.003928        0.006053   \n",
       "1      12.527428      0.845753         0.012635        0.006323   \n",
       "2      14.899685      0.807141         0.010755        0.006318   \n",
       "3      16.358911      0.778196         0.012321        0.007329   \n",
       "\n",
       "  param_hidden_layer_sizes                                  params  \\\n",
       "0                (4, 7, 4)       {'hidden_layer_sizes': (4, 7, 4)}   \n",
       "1               (6, 12, 6)      {'hidden_layer_sizes': (6, 12, 6)}   \n",
       "2             (8, 8, 8, 8)    {'hidden_layer_sizes': (8, 8, 8, 8)}   \n",
       "3           (5, 15, 15, 5)  {'hidden_layer_sizes': (5, 15, 15, 5)}   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0           0.927809           0.897611           0.770282           0.824594   \n",
       "1           0.976187           0.968525           0.963642           0.943434   \n",
       "2           0.977539           0.971680           0.983023           0.968975   \n",
       "3           0.982197           0.981295           0.981746           0.975436   \n",
       "\n",
       "   split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0           0.911508         0.866361        0.059609                4  \n",
       "1           0.968299         0.964017        0.011050                3  \n",
       "2           0.946289         0.969501        0.012580                2  \n",
       "3           0.935021         0.971139        0.018226                1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7dc84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_no_attack = RandomForestClassifier (max_depth = 30, n_estimators = 50 ,verbose=True)\n",
    "clf_no_attack.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39326f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict=clf.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "print(pd.DataFrame(cm))\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c353e3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {\"n_estimators\":(10,20,50,100), \"max_depth\":(3,5,8,16)}\n",
    "grid = GridSearchCV(estimator=clf_no_attack, param_grid=params, scoring = 'accuracy')\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840722f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58c6caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf92e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_image_test = No_Attack_frame.iloc[:,[1,2,3,4]].values  \n",
    "i=8\n",
    "y=X_image_test[1024*i:1024*(i+1)]\n",
    "y_predict_image=clf.predict(y)\n",
    "y_predict_image[y_predict_image>0]=255\n",
    "y_predict_image=y_predict_image.reshape(32,32)\n",
    "img = Image.fromarray(y_predict_image.astype(np.uint8))\n",
    "img.show()\n",
    "img.save(\"W_R/WR1.tif\")\n",
    "W=WaterMarkLoader.load(\"Water Mark Image/WaterMarkRandom.jpg\")\n",
    "WR=WaterMarkLoader.load(\"W_R/WR1.tif\")\n",
    "print(\"побитовое сравнение = \", pobitovo_sravnenie_WaterMark(W,WR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b433d734",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_averege= \"feature_vec/AverageAttack.txt\"\n",
    "path_histogram=\"feature_vec/HistogramAttack.txt\"\n",
    "path_GammaCor=  \"feature_vec/GammaCorrection.txt\"\n",
    "path_JPEG50= \"feature_vec/JPEG50.txt\"\n",
    "path_median=  \"feature_vec/medianAttack.txt\"\n",
    "path_saltpaper=\"feature_vec/SaltPaperAttack.txt\"\n",
    "path_snarpness= \"feature_vec/Sharpness.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87bc6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "averege=pd.read_csv(path_averege) \n",
    "histogram=pd.read_csv(path_histogram)\n",
    "GammaCor=pd.read_csv(path_GammaCor) \n",
    "JPEG50=pd.read_csv(path_JPEG50) \n",
    "median=pd.read_csv(path_median) \n",
    "saltpaper=pd.read_csv(path_saltpaper)\n",
    "snarpness=pd.read_csv(path_snarpness) \n",
    "frames = [averege, histogram, GammaCor , JPEG50, median,saltpaper ,snarpness]\n",
    "DataFrame = pd.concat(frames)\n",
    "del DataFrame['Unnamed: 0']\n",
    "del saltpaper['Unnamed: 0']\n",
    "del averege['Unnamed: 0']\n",
    "del histogram['Unnamed: 0']\n",
    "del GammaCor['Unnamed: 0']\n",
    "del median['Unnamed: 0']\n",
    "del JPEG50['Unnamed: 0']\n",
    "del snarpness['Unnamed: 0']\n",
    "\n",
    "DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfca419",
   "metadata": {},
   "source": [
    "### тестирую модель обученную на noattack подсовываю атакованные изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d0eba7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NO_AttackModel_Test(saltpaper,\"W_R/WR_SP.tif\", \"ПРОВЕРКА ДЛЯ АТАКИ СОЛЬ-ПЕРЕЦ. МОДЕЛЬ ОБУЧЕНА НА НЕАТАКОВАННЫХ ИЗОБРАЖЕНИЯХ\",clf_no_attack,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486e7a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attack_Dota_SP = pd.read_csv('feature_vec/SaltPaperAttackDota.txt') # ПАРАМЕТР НА ВХОД ФУНКЦИИ\n",
    "one_WM(Attack_Dota_SP ,\"W_R/WR_SP_Dota.tif\",\"Water Mark Image/waterMark3.jpg\" , clf_no_attack )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211ace01",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_AttackModel_Test(JPEG50,\"W_R/WR_JPEG50.tif\", \"ПРОВЕРКА ДЛЯ АТАКИ JPEG50. МОДЕЛЬ ОБУЧЕНА НА НЕАТАКОВАННЫХ ИЗОБРАЖЕНИЯХ\",clf_no_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630ba6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attack_Dota_jpeg = pd.read_csv('feature_vec/JPEG50Dota.txt') # ПАРАМЕТР НА ВХОД ФУНКЦИИ\n",
    "one_WM(Attack_Dota_jpeg ,\"W_R/WR_jpeg_Dota.tif\",\"Water Mark Image/waterMark3.jpg\" ,clf_no_attack )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3bda18",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_AttackModel_Test(averege,\"W_R/WR_averege.tif\", \"ПРОВЕРКА ДЛЯ АТАКИ averege - среднее. МОДЕЛЬ ОБУЧЕНА НА НЕАТАКОВАННЫХ ИЗОБРАЖЕНИЯХ\",clf_no_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5318a487",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attack_Dota_Average = pd.read_csv('feature_vec/AverageAttackDota.txt') # ПАРАМЕТР НА ВХОД ФУНКЦИИ\n",
    "one_WM(Attack_Dota_Average ,\"W_R/WR_Average_Dota.tif\",\"Water Mark Image/waterMark3.jpg\",clf_no_attack )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aea1974",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_AttackModel_Test(histogram,\"W_R/WR_histogram.tif\", \"ПРОВЕРКА ДЛЯ АТАКИ histogram. МОДЕЛЬ ОБУЧЕНА НА НЕАТАКОВАННЫХ ИЗОБРАЖЕНИЯХ\",clf_no_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad455ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attack_Dota_Histogram = pd.read_csv('feature_vec/HistogramAttackDota.txt') # ПАРАМЕТР НА ВХОД ФУНКЦИИ\n",
    "one_WM(Attack_Dota_Histogram ,\"W_R/WR_Histogram_Dota.tif\",\"Water Mark Image/waterMark3.jpg\",clf_no_attack )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a69535",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_AttackModel_Test(GammaCor,\"W_R/WR_GammaCor.tif\", \"ПРОВЕРКА ДЛЯ АТАКИ GammaCor. МОДЕЛЬ ОБУЧЕНА НА НЕАТАКОВАННЫХ ИЗОБРАЖЕНИЯХ\",clf_no_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87b8f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attack_Dota_GammaCorrection= pd.read_csv('feature_vec/GammaCorrectionCross.txt') # ПАРАМЕТР НА ВХОД ФУНКЦИИ\n",
    "one_WM(Attack_Dota_GammaCorrection ,\"W_R/WR_GammaCorrection.tif\",\"Water Mark Image/waterMark3.jpg\",clf_no_attack )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9258322",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_AttackModel_Test(median,\"W_R/WR_median.tif\", \"ПРОВЕРКА ДЛЯ АТАКИ median. МОДЕЛЬ ОБУЧЕНА НА НЕАТАКОВАННЫХ ИЗОБРАЖЕНИЯХ\",clf_no_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d74469",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attack_Cross_medianAttack= pd.read_csv('feature_vec/medianAttackCross.txt') # ПАРАМЕТР НА ВХОД ФУНКЦИИ\n",
    "one_WM(Attack_Cross_medianAttack ,\"W_R/medianAttackCross.tif\",\"Water Mark Image/waterMark2.jpg\",clf_no_attack )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_AttackModel_Test(snarpness,\"W_R/WR_snarpness.tif\", \"ПРОВЕРКА ДЛЯ АТАКИ snarpness. МОДЕЛЬ ОБУЧЕНА НА НЕАТАКОВАННЫХ ИЗОБРАЖЕНИЯХ\",clf_no_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f92dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attack_Cross_Sharpness= pd.read_csv('feature_vec/SharpnessCross.txt') # ПАРАМЕТР НА ВХОД ФУНКЦИИ\n",
    "one_WM(Attack_Cross_Sharpness ,\"W_R/SharpnessCross.tif\",\"Water Mark Image/waterMark2.jpg\",clf_no_attack )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd455357",
   "metadata": {},
   "source": [
    "### тестирую модель обученную на attack image подсовываю атакованные изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01d42e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = DataFrame.iloc[:,[1,2,3,4]].values  \n",
    "Y=DataFrame.iloc[:,[0]].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47132d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_attack = MLPClassifier(hidden_layer_sizes = (20,15,10,5) , max_iter=1000 ,early_stopping=True , verbose=True)\n",
    "clf_attack.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b713193",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict=clf_attack.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "print(pd.DataFrame(cm))\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b614fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_AttackModel_Test(saltpaper,\"W_R/WR_SP.tif\", \"ПРОВЕРКА ДЛЯ АТАКИ СОЛЬ-ПЕРЕЦ. МОДЕЛЬ ОБУЧЕНА НА НЕАТАКОВАННЫХ ИЗОБРАЖЕНИЯХ\",clf_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77526a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attack_Dota_SP = pd.read_csv('feature_vec/SaltPaperAttackDota.txt') # ПАРАМЕТР НА ВХОД ФУНКЦИИ\n",
    "one_WM(Attack_Dota_SP ,\"W_R/WR_SP_Dota.tif\",\"Water Mark Image/waterMark3.jpg\",clf_attack )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3b457f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NO_AttackModel_Test(averege,\"W_R/WR_averege.tif\", \"ПРОВЕРКА ДЛЯ АТАКИ averege - среднее. МОДЕЛЬ ОБУЧЕНА НА НЕАТАКОВАННЫХ ИЗОБРАЖЕНИЯХ\",clf_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attack_Dota_Average = pd.read_csv('feature_vec/AverageAttackDota.txt') # ПАРАМЕТР НА ВХОД ФУНКЦИИ\n",
    "one_WM(Attack_Dota_Average ,\"W_R/WR_Average_Dota.tif\",\"Water Mark Image/waterMark3.jpg\",clf_attack )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bf146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_AttackModel_Test(histogram,\"W_R/WR_histogram.tif\", \"ПРОВЕРКА ДЛЯ АТАКИ histogram. МОДЕЛЬ ОБУЧЕНА НА НЕАТАКОВАННЫХ ИЗОБРАЖЕНИЯХ\",clf_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ab7f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attack_Dota_Histogram = pd.read_csv('feature_vec/HistogramAttackDota.txt') # ПАРАМЕТР НА ВХОД ФУНКЦИИ\n",
    "one_WM(Attack_Dota_Histogram ,\"W_R/WR_Histogram_Dota.tif\",\"Water Mark Image/waterMark3.jpg\",clf_attack )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526e8bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_AttackModel_Test(GammaCor,\"W_R/WR_GammaCor.tif\", \"ПРОВЕРКА ДЛЯ АТАКИ GammaCor. МОДЕЛЬ ОБУЧЕНА НА НЕАТАКОВАННЫХ ИЗОБРАЖЕНИЯХ\",clf_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a7e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attack_Dota_GammaCorrection= pd.read_csv('feature_vec/GammaCorrectionCross.txt') # ПАРАМЕТР НА ВХОД ФУНКЦИИ\n",
    "one_WM(Attack_Dota_GammaCorrection ,\"W_R/WR_GammaCorrection.tif\",\"Water Mark Image/waterMark3.jpg\",clf_attack )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa516f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_AttackModel_Test(median,\"W_R/WR_median.tif\", \"ПРОВЕРКА ДЛЯ АТАКИ median. МОДЕЛЬ ОБУЧЕНА НА НЕАТАКОВАННЫХ ИЗОБРАЖЕНИЯХ\",clf_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64402a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attack_Cross_medianAttack= pd.read_csv('feature_vec/medianAttackCross.txt') # ПАРАМЕТР НА ВХОД ФУНКЦИИ\n",
    "one_WM(Attack_Cross_medianAttack ,\"W_R/medianAttackCross.tif\",\"Water Mark Image/waterMark2.jpg\",clf_attack )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ccd601",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_AttackModel_Test(snarpness,\"W_R/WR_snarpness.tif\", \"ПРОВЕРКА ДЛЯ АТАКИ snarpness. МОДЕЛЬ ОБУЧЕНА НА НЕАТАКОВАННЫХ ИЗОБРАЖЕНИЯХ\",clf_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f5b146",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attack_Cross_Sharpness= pd.read_csv('feature_vec/SharpnessCross.txt') # ПАРАМЕТР НА ВХОД ФУНКЦИИ\n",
    "one_WM(Attack_Cross_Sharpness ,\"W_R/SharpnessCross.tif\",\"Water Mark Image/waterMark2.jpg\",clf_attack )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe25c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_AttackModel_Test(JPEG50,\"W_R/WR_JPEG50.tif\", \"ПРОВЕРКА ДЛЯ АТАКИ JPEG50. МОДЕЛЬ ОБУЧЕНА НА НЕАТАКОВАННЫХ ИЗОБРАЖЕНИЯХ\",clf_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b587a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attack_Dota_jpeg = pd.read_csv('feature_vec/JPEG50Dota.txt') # ПАРАМЕТР НА ВХОД ФУНКЦИИ\n",
    "one_WM(Attack_Dota_jpeg ,\"W_R/WR_jpeg_Dota.tif\",\"Water Mark Image/waterMark3.jpg\" ,clf_attack )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1ca33b",
   "metadata": {},
   "source": [
    "#  Обучаю сеть на одной конкретной атаке и смотрю на результат"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f89886b",
   "metadata": {},
   "source": [
    "### Медианная фильтрация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c19c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "median=pd.read_csv(path_median) \n",
    "No_Attack_frame = pd.read_csv('feature_vec/NO_Attack.txt') # ПАРАМЕТР НА ВХОД ФУНКЦИИ\n",
    "frames = [ median , No_Attack_frame ]\n",
    "DataFrame = pd.concat(frames)\n",
    "del DataFrame['Unnamed: 0']\n",
    "del median['Unnamed: 0']\n",
    "DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a63bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = DataFrame.iloc[:,[1,2,3,4]].values  \n",
    "Y=DataFrame.iloc[:,[0]].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.35, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa01ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_specific_attack = MLPClassifier(hidden_layer_sizes = (20,15,10,5) , max_iter=1000 ,early_stopping=True , verbose=True)\n",
    "clf_specific_attack.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02436ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_AttackModel_Test(median,\"W_R/WR_median.tif\", \"ПРОВЕРКА ДЛЯ АТАКИ median. МОДЕЛЬ ОБУЧЕНА НА НЕАТАКОВАННЫХ ИЗОБРАЖЕНИЯХ\",clf_specific_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d040f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attack_Cross_medianAttack= pd.read_csv('feature_vec/medianAttackCross.txt') # ПАРАМЕТР НА ВХОД ФУНКЦИИ\n",
    "one_WM(Attack_Cross_medianAttack ,\"W_R/medianAttackCross.tif\",\"Water Mark Image/waterMark2.jpg\",clf_specific_attack )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9996115a",
   "metadata": {},
   "source": [
    "### Среднее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80311eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "averege=pd.read_csv(path_averege) \n",
    "No_Attack_frame = pd.read_csv('feature_vec/NO_Attack.txt') # ПАРАМЕТР НА ВХОД ФУНКЦИИ\n",
    "frames = [ averege , No_Attack_frame ]\n",
    "DataFrame = pd.concat(frames)\n",
    "del DataFrame['Unnamed: 0']\n",
    "del averege['Unnamed: 0']\n",
    "DataFrame\n",
    "\n",
    "X = DataFrame.iloc[:,[1,2,3,4]].values  \n",
    "Y=DataFrame.iloc[:,[0]].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.35, random_state=42)\n",
    "\n",
    "clf_specific_attack = MLPClassifier(hidden_layer_sizes = (20,15,10,5) , max_iter=1000 ,early_stopping=True , verbose=True)\n",
    "clf_specific_attack.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea17271",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_AttackModel_Test(median,\"W_R/WR_average.tif\", \"ПРОВЕРКА ДЛЯ АТАКИ average МОДЕЛЬ ОБУЧЕНА НА НЕАТАКОВАННЫХ ИЗОБРАЖЕНИЯХ\",clf_specific_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38189b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attack_Dota_Average = pd.read_csv('feature_vec/AverageAttackDota.txt') # ПАРАМЕТР НА ВХОД ФУНКЦИИ\n",
    "one_WM(Attack_Dota_Average ,\"W_R/WR_Average_Dota.tif\",\"Water Mark Image/waterMark3.jpg\",clf_specific_attack )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924d490e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
